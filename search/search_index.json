{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Jaxnasium: A Lightweight Utility Library for JAX-based RL Projects","text":"<p>Jaxnasium lets you</p> <ol> <li>\ud83d\udd79\ufe0f Import your favourite environments from various libraries with a single API and automatically wrap them to a common standard.</li> <li>\ud83d\ude80 Bootstrap new JAX RL projects with a single CLI command and get started instantly with a complete codebase.</li> <li>\ud83e\udd16 Jaxnasium comes equiped with standard general RL implementations based on a near-single-file philosophy. You can either import these as off-the-shelf algorithms or copy over the code and tweak them for your problem. These algorithms follow the ideas of PureJaxRL for extremely fast end-to-end RL training in JAX.</li> </ol>"},{"location":"#getting-started","title":"\ud83d\ude80 Getting started","text":"<p>Jaxnasium lets you bootstrap your new reinforcement learning projects directly from the command line. As such, for new projects, the easiest way to get started is via uv:</p> <pre><code>uvx jaxnasium &lt;projectname&gt;\nuv run example_train.py\n\n# ... or via pipx\npipx run jaxnasium &lt;projectname&gt;\n# activate a virtual environment in your preferred way, e.g. conda\npython example_train.py\n</code></pre> <p>This will set up a Python project folder structure with (optionally) an environment template and (optionally) algorithm code for you to tailor to your problem.</p> <p>For existing projects, you can simply install Jaxnasium via <code>pip</code> and import the required functionality.</p> <pre><code>pip install jaxnasium\n</code></pre> <pre><code>import jax\nimport jaxnasium as jym\nfrom jaxnasium.algorithms import PPO\n\nenv = jym.make(\"CartPole-v1\")\nenv = jaxnasium.LogWrapper(env)\nrng = jax.random.PRNGKey(0)\nagent = PPO(total_timesteps=5e5, learning_rate=2.5e-3)\nagent = agent.train(rng, env)\n</code></pre>"},{"location":"#environments","title":"\ud83c\udfe0 Environments","text":"<p>Jaxnasium is not aimed at delivering a full environment suite. However, it does come equipped with a <code>jym.make(...)</code> command to import environments from existing suites (provided that these are installed) and wrap them appropriately to the Jaxnasium API standard. For example, using environments from Gymnax:</p> <pre><code>import jaxnasium as jym\nfrom jaxnasium.algorithms import PPO\nimport jax\n\nenv = jym.make(\"Breakout-MinAtar\")\nenv = jym.FlattenObservationWrapper(env)\nenv = jym.LogWrapper(env)\n\nagent = PPO(**some_good_hyperparameters)\nagent = agent.train(jax.random.PRNGKey(0), env)\n\n# &gt; Using an environment from Gymnax via gymnax.make(Breakout-MinAtar).\n# &gt; Wrapping Gymnax environment with GymnaxWrapper\n# &gt;  Disable this behavior by passing wrapper=False\n# &gt; Wrapping environment in VecEnvWrapper\n# &gt; ... training results\n</code></pre> <p>Info</p> <p>For convenience, Jaxnasium does include the 5 classic-control environments.</p> <p>See the Environments page for a complete list of available environments.</p>"},{"location":"#environment-api","title":"Environment API","text":"<p>The Jaxnasium API stays close to the somewhat established Gymnax API for the <code>reset()</code> and <code>step()</code> functions, but allows for truncated episodes in a manner closer to Gymnasium.</p> <pre><code>env = jym.make(...)\n\nobs, env_state = env.reset(key) # &lt;-- Mirroring Gymnax\n\n# env.step(): Gymnasium Timestep tuple with state information\n(obs, reward, terminated, truncated, info), env_state = env.step(key, state, action)\n</code></pre>"},{"location":"#algorithms","title":"\ud83e\udd16 Algorithms","text":"<p>Algorithms in <code>jaxnasium.algorithms</code> are built following a near-single-file implementation philosophy in mind. In contrast to implementations in CleanRL or PureJaxRL, Jaxnasium algorithms are built in Equinox and follow a class-based design with a familiar Stable-Baselines API. </p> <pre><code>from jaxnasium.algorithms import PPO\nimport jax\n\nenv = ...\nagent = PPO(**some_good_hyperparameters)\nagent = agent.train(jax.random.PRNGKey(0), env)\n</code></pre> <p>See the Algorithms for more details on the included algorithms..</p> Algorithm Multi-Agent<sup>1</sup> Observation Spaces Action Spaces Composite (nested) Spaces<sup>2</sup> PPO \u2705 <code>Box</code>, <code>Discrete</code>, <code>MultiDiscrete</code> <code>Box</code>, <code>Discrete</code>, <code>MultiDiscrete</code> \u2705 DQN \u2705 <code>Box</code>, <code>Discrete</code>, <code>MultiDiscrete</code> <code>Discrete</code>, <code>MultiDiscrete</code><sup>3</sup> \u2705 PQN \u2705 <code>Box</code>, <code>Discrete</code>, <code>MultiDiscrete</code> <code>Discrete</code>, <code>MultiDiscrete</code><sup>3</sup> \u2705 SAC \u2705 <code>Box</code>, <code>Discrete</code>, <code>MultiDiscrete</code> <code>Box</code>, <code>Discrete</code>, <code>MultiDiscrete</code> \u2705 <sup>1</sup> All algorithms support automatic multi-agent transformation through the <code>auto_upgrade_multi_agent</code> parameter. See Multi-Agent for more information. <sup>2</sup> Algorithms support composite (nested) spaces. See Spaces for more information. <sup>3</sup> MultiDiscrete action spaces in PQN and DQN are only supported when flattening to a Discrete action space. E.g. via the <code>FlattenActionSpaceWrapper</code>."},{"location":"algorithms/Algorithms/","title":"RL Algorithms","text":"<p>Jaxnasium provides a suite of reinforcement learning algorithms. Currently, a small set of algorithms are implemented. More may be added in the future, but the current objective is not to span a wide range of RL algorithms. </p>"},{"location":"algorithms/Algorithms/#algorithm-overview","title":"Algorithm Overview","text":"<p>Jaxnasium algorithms are primarily inspired by CleanRL and PureJaxRL and therefor follow a near-single-file implementation philosophy. However, Jaxnasium algorithms are built in Equinox and follow a class-based design with a familiar Stable-Baselines API. </p> <p>All algorithms inherit from the <code>RLAlgorithm</code> abstract base class, which primarily defines a minimal common interface for all algorithms, contains a standard evaluation loop and handles multi-agent support. All training logic is implemented in the algorithms themselves.</p>"},{"location":"algorithms/Algorithms/#available-algorithms","title":"Available Algorithms","text":"Algorithm Multi-Agent<sup>1</sup> Observation Spaces Action Spaces Composite (nested) Spaces<sup>2</sup> PPO \u2705 <code>Box</code>, <code>Discrete</code>, <code>MultiDiscrete</code> <code>Box</code>, <code>Discrete</code>, <code>MultiDiscrete</code> \u2705 DQN \u2705 <code>Box</code>, <code>Discrete</code>, <code>MultiDiscrete</code> <code>Discrete</code>, <code>MultiDiscrete</code><sup>3</sup> \u2705 PQN \u2705 <code>Box</code>, <code>Discrete</code>, <code>MultiDiscrete</code> <code>Discrete</code>, <code>MultiDiscrete</code><sup>3</sup> \u2705 SAC \u2705 <code>Box</code>, <code>Discrete</code>, <code>MultiDiscrete</code> <code>Box</code>, <code>Discrete</code>, <code>MultiDiscrete</code> \u2705 <sup>1</sup> All algorithms support automatic multi-agent transformation through the <code>auto_upgrade_multi_agent</code> parameter. See Multi-Agent for more information. <sup>2</sup> Algorithms support composite (nested) spaces. See Spaces for more information. <sup>3</sup> MultiDiscrete action spaces in PQN and DQN are only supported when flattening to a Discrete action space. E.g. via the <code>FlattenActionSpaceWrapper</code>."},{"location":"algorithms/Algorithms/#key-features-across-all-algorithms","title":"Key Features Across All Algorithms","text":"<ul> <li>Automatic Multi-Agent Support: All algorithms automatically transform to handle multi-agent environments</li> <li>Flexible Action Spaces: Support for discrete, continuous, and mixed action spaces. Algorihms deal with any composite (pytree of) spaces.</li> <li>PureJaxRL Training: Training logic, when used with a JIT-compatible environment, is fully JIT-compatible, allowing for extremely fast end-to-end training in JAX.</li> <li>Modular Design: Near-single-file implementations for easy understanding and modification</li> <li>Built-in Normalization: Optional observation and reward normalization.</li> <li>Logging: Optional logging during training built-in.</li> </ul>"},{"location":"algorithms/Algorithms/#action-and-observation-space-support-details","title":"Action and Observation Space Support Details","text":"<p>All algorithms in Jaxnasium support composite observation and action spaces through PyTree structures. When observation or action spaces are defined as PyTrees of spaces (e.g., dictionaries, tuples, or nested combinations), the algorithms automatically handle the structured data flow. The neural networks are designed to process PyTree inputs and outputs seamlessly.This design allows algorithms to work with complex environments without requiring manual preprocessing or postprocessing of the data. </p>"},{"location":"algorithms/Algorithms/#multi-agent-capabilities","title":"Multi-Agent Capabilities","text":"<p>Every algorithm in Jaxnasium includes automatic multi-agent support through function transformations. When you provide a multi-agent environment, the algorithm automatically:</p> <ul> <li>Transforms some methods to act on the first level of the PyTree structure of the inputs. Thereby performing per-agent operations.</li> <li>Algorithms can be designed as a single-agent algorithm, and handle multi-agent scenarios seamlessly.</li> <li>Homogeneous and heterogeneous agent scenarios are supported. Homogeneous agent operations may run in parallel. </li> </ul> <p>For more information, see the Multi-Agent documentation.</p>"},{"location":"algorithms/Algorithms/#getting-started","title":"Getting Started","text":"<p>Each algorithm follows a consistent interface:</p> <pre><code>import jaxnasium as jym\nfrom jaxnasium.algorithms import PPO\n\n# Create algorithm instance\nalgorithm = PPO(\n    learning_rate=3e-4,\n    total_timesteps=1_000_000,\n    num_envs=8\n)\n\n# Train on environment\ntrained_algorithm = algorithm.train(key, env)\n\n# Evaluate\nrewards = trained_algorithm.evaluate(key, env, num_episodes=10) # jnp.array of shape (num_episodes,)\n</code></pre> <p>The algorithms are designed to work seamlessly with any Jaxnasium environment, automatically adapting to the environment's observation and action spaces, and scaling to multi-agent scenarios when needed.</p>"},{"location":"algorithms/Multi-Agent/","title":"Multi-Agent RL","text":"<p>Multi-agent reinforcement learning is a core pillar of Jaxnasium's design philosophy. Rather than maintaining separate APIs for single-agent and multi-agent settings, Jaxnasium unifies both paradigms through PyTrees and function transformations.</p>"},{"location":"algorithms/Multi-Agent/#expected-pytree-structures","title":"Expected PyTree Structures","text":"<p>Jaxnasium's Multi-Agent design is heavily based on JAX's PyTrees. PyTrees allow us to (nested) data structures (like dictionaries, lists, tuples) in a way that JAX can efficiently process and transform. This becomes the perfect abstraction for multi-agent scenarios where we need to handle:</p> <ul> <li>Observations: <code>{\"agent_0\": obs_0, \"agent_1\": obs_1, ...}</code></li> <li>Actions: <code>{\"agent_0\": action_0, \"agent_1\": action_1, ...}</code></li> <li>Rewards: <code>{\"agent_0\": reward_0, \"agent_1\": reward_1, ...}</code></li> </ul> <p>Essentially, Jaxnasium enviroments with <code>multi_agent=True</code> are expected to have an <code>action_space</code> and <code>observation_space</code> that are PyTrees of spaces. The first level of the PyTree is the agent dimension. Similarly, the reward function and, optionally, the  termination and truncation flag should also return PyTrees with the same first-level structure. This is similar to the API already set out by JaxMARL, but Jaxnasium allows any PyTree structure of agents.</p> <p>All elements below the first level of the PyTree can be arbitrary structures, including more nested PyTrees.</p> <pre><code># Homogeneous agents\nenv.action_space = {\n    \"agent_0\": Discrete(2), \"agent_1\": Discrete(2)\n}\nenv.observation_space = {\n    \"agent_0\": Box(low=0, high=1, shape=(3,)), \"agent_1\": Box(low=0, high=1, shape=(3,))\n}\nreward = {\n    \"agent_0\": -1, \"agent_1\": 1\n}\n\n# Heterogeneous agents\nenv.action_space = {\n    \"agent_0\": Discrete(2), \"agent_1\": MultiDiscrete(2, 3)\n}\nenv.observation_space = {\n    \"agent_0\": Box(low=0, high=1, shape=(3,)), \"agent_1\": Box(low=0, high=1, shape=(8,))\n}\nreward = {\n    \"agent_0\": -1, \"agent_1\": 1\n}\n\n# Heterogeneous agents in a list with nested PyTree actions\nenv.action_space = [\n    {\"position\": Discrete(2), \"velocity\": Discrete(2)}, \n    {\"action\": MultiDiscrete(2, 3)}\n]\nenv.observation_space = [\n    {\"xy\": Box(low=0, high=1, shape=(2,)), \"velocity\": Box(low=0, high=1, shape=(1,))},\n    Discrete(3)\n]\nreward = [\n    -1, 1\n]\n</code></pre> <p>Enforcement</p> <p>Jaxnasium Environments do not enforce this Multi-Agent structure. It is however recommended, and expected by the Jaxnasium algorithms.</p>"},{"location":"algorithms/Multi-Agent/#why-this-is-useful","title":"Why this is useful","text":"<p>The core idea here is that we can write single-agent algorithms that can easily transition to multi-agent settings via  Jax's built-in PyTree operations.</p>"},{"location":"algorithms/Multi-Agent/#the-transform_multi_agent-decorator","title":"The <code>transform_multi_agent</code> Decorator","text":"<p>The <code>transform_multi_agent</code> decorator is the magic that makes single-agent algorithms automatically work with multi-agent environments.</p>"},{"location":"algorithms/Multi-Agent/#core-mechanism","title":"Core Mechanism","text":"<pre><code>@transform_multi_agent\ndef get_action(key, agent_state, observation):\n    action_dist = agent_state.actor(observation)\n    return action_dist.sample(seed=key)\n</code></pre> <p>When this function is called with multi-agent data:</p> <pre><code># Multi-agent inputs\nagent_states = {\"agent_0\": state_0, \"agent_1\": state_1}\nobservations = {\"agent_0\": obs_0, \"agent_1\": obs_1}\nkey = jax.random.PRNGKey(42) # Key is (optionally) automatically split over the agents.\n\n# The decorator automatically handles the transformation\nactions = get_action(key, agent_states, observations)\n# Result: {\"agent_0\": action_0, \"agent_1\": action_1}\n</code></pre> <ul> <li> <p>Argument Structure: The first argument of the function is assumed to have first-level PyTree structure of agents. The remaining arguments that are not provided in <code>shared_argnames</code> are assumed to have the same first-level PyTree structure. functions in <code>shared_argnames</code> will be shared across agents.</p> </li> <li> <p>Key Splitting: Optionally, PRNG keys can be provided as a single key, and will automatically be split accross the first-level PyTree structure of the first argument.</p> </li> <li> <p>Automatic Shared Arguments Detection: Optionally, rather than explicitly providing the <code>shared_argnames</code> argument, the decorator can automatically detect shared arguments based on the function signature. Arguments that do not have the same first-level PyTree structure as the first argument are assumed to be shared.</p> </li> <li> <p>Homogeneous Agents: Uses <code>jax.vmap</code> for maximum efficiency when all agents have identical structures</p> </li> <li> <p>Heterogeneous Agents: Uses <code>jax.tree.map</code> for flexible handling of different agent types</p> </li> <li> <p>Automatic Transposition: For <code>Transition</code> objects (replay buffer data), the decorator automatically transposes the data structure to be compatible with the function signature.</p> </li> </ul>"},{"location":"algorithms/Multi-Agent/#the-__make_multi_agent__-method","title":"The <code>__make_multi_agent__</code> Method","text":"<p>The <code>RLAlgorithm.__make_multi_agent__</code> method is the bridge that connects single-agent algorithms to multi-agent environments. It will apply the <code>transform_multi_agent</code> decorator to specified methods and return a new instance of the algorithm that is in multi-agent mode. By default, the transformed methods are:</p> <ul> <li><code>get_action</code></li> <li><code>get_value</code></li> <li><code>_update_agent_state</code></li> <li><code>_make_agent_state</code></li> <li><code>_postprocess_rollout</code></li> </ul>"},{"location":"algorithms/Multi-Agent/#automatic-upgrade-process","title":"Automatic Upgrade Process","text":"<p>When an algorithm encounters a multi-agent environment:</p> <pre><code>def init_state(self, key: PRNGKeyArray, env: Environment) -&gt; \"PPO\":\n    if getattr(env, \"multi_agent\", False) and self.auto_upgrade_multi_agent:\n        self = self.__make_multi_agent__()  # Automatic upgrade!\n</code></pre>"},{"location":"algorithms/Multi-Agent/#pytree-operations","title":"PyTree Operations","text":"<p>The following functions are commonly used to handle multi-agent data:</p>"},{"location":"algorithms/Multi-Agent/#map_one_level","title":"<code>map_one_level</code>","text":"<p>Maps a function over the first level of a PyTree structure: <pre><code># Applies function to each agent's data\nresult = map_one_level(agent_function, agent_data)\n</code></pre></p>"},{"location":"algorithms/Multi-Agent/#stack-and-unstack","title":"<code>stack</code> and <code>unstack</code>","text":"<p>Efficiently converts between agent-wise and batch-wise representations: <pre><code># Convert agent-wise to batch-wise for vmap\nstacked = stack(agent_data)  # {\"agent_0\": data_0, \"agent_1\": data_1} -&gt; batched_data\nresult = jax.vmap(function)(stacked)\n# Convert back to agent-wise\nunstacked = unstack(result, structure=original_structure)\n</code></pre></p>"},{"location":"algorithms/Multi-Agent/#documentation","title":"Documentation","text":""},{"location":"algorithms/Multi-Agent/#src.jaxnasium.algorithms.utils.transform_multi_agent","title":"<code>src.jaxnasium.algorithms.utils.transform_multi_agent(func: Optional[Callable] = None, *, shared_argnames: Optional[List[str]] = None, auto_split_keys: bool = True, auto_transpose_transitions: bool = True) -&gt; Callable</code>","text":"<p>Transformation docorator to handle multi-agent settings. Essentially, this function either applies a vmap over the agents (if the agent are homogeneous) or applies a <code>jax.tree.map</code> over the first level of the PyTree structure of the arguments.</p> <p>Essentially, this transformation allows for writing single-agent functions that can automatically be upgraded to multi-agent settings.</p> <p>Arguments:</p> <ul> <li><code>func</code>: The function to be transformed. If None, returns a decorator.</li> <li><code>shared_argnames</code>: An optional list of argument names that are shared across agents. If None, the first (non-PRNGKey) argument is     assumed to be a per-agent argument. All arguments with the same first-level PyTree structure are also considered per-agent arguments.     The rest are considered shared arguments. PRNG keys that are provided as arguments are automatically split over agents.</li> <li><code>auto_split_keys</code>: When enabled, provided single PRNGKeys are automatically split over the same structure as the first argument.</li> <li><code>auto_split_transitions</code>: When enabled, <code>Transition</code> objects are automatically transposed before being passed to the function.         After the function call, the <code>Transition</code> object is reconstructed into the original structure.</li> </ul> <p>Example: <pre><code>&gt;&gt;&gt; @transform_multi_agent\n... def get_action(key, agent, observation):\n...     action_dist = agent.actor(observation)\n...     return action_dist.sample(seed=key)\n\n# Usage:\n&gt;&gt;&gt; models = {\"agent0\": model1, \"agent1\": model2}\n&gt;&gt;&gt; agent_observations = {\"agent0\": obs0, \"agent1\": obs1} # &lt;-- Same first-level structure as `models`\n&gt;&gt;&gt; key = jax.random.PRNGKey(42) # &lt;-- `key` inputs are automatically split over agents\n&gt;&gt;&gt; actions = get_action(agent_states, agent_observations, key)\n&gt;&gt;&gt; # Result: {\"agent0\": action0, \"agent1\": action1}\n</code></pre></p>"},{"location":"algorithms/_Algorithm-Table/","title":"Algorithm Table","text":"Algorithm Multi-Agent<sup>1</sup> Observation Spaces Action Spaces Composite (nested) Spaces<sup>2</sup> PPO \u2705 <code>Box</code>, <code>Discrete</code>, <code>MultiDiscrete</code> <code>Box</code>, <code>Discrete</code>, <code>MultiDiscrete</code> \u2705 DQN \u2705 <code>Box</code>, <code>Discrete</code>, <code>MultiDiscrete</code> <code>Discrete</code>, <code>MultiDiscrete</code><sup>3</sup> \u2705 PQN \u2705 <code>Box</code>, <code>Discrete</code>, <code>MultiDiscrete</code> <code>Discrete</code>, <code>MultiDiscrete</code><sup>3</sup> \u2705 SAC \u2705 <code>Box</code>, <code>Discrete</code>, <code>MultiDiscrete</code> <code>Box</code>, <code>Discrete</code>, <code>MultiDiscrete</code> \u2705 <sup>1</sup> All algorithms support automatic multi-agent transformation through the <code>auto_upgrade_multi_agent</code> parameter. See Multi-Agent for more information. <sup>2</sup> Algorithms support composite (nested) spaces. See Spaces for more information. <sup>3</sup> MultiDiscrete action spaces in PQN and DQN are only supported when flattening to a Discrete action space. E.g. via the <code>FlattenActionSpaceWrapper</code>."},{"location":"algorithms/misc/Distributions/","title":"Distributions","text":""},{"location":"algorithms/misc/Distributions/#src.jaxnasium.algorithms.utils.DistraxContainer","title":"<code>src.jaxnasium.algorithms.utils.DistraxContainer</code>","text":"<p>Container for (possibly nested as PyTrees) distrax distributions.</p>"},{"location":"algorithms/misc/Distributions/#src.jaxnasium.algorithms.utils.DistraxContainer.sample","title":"<code>sample(*, seed)</code>","text":""},{"location":"algorithms/misc/Distributions/#src.jaxnasium.algorithms.utils.DistraxContainer.log_prob","title":"<code>log_prob(value)</code>","text":""},{"location":"algorithms/misc/Distributions/#src.jaxnasium.algorithms.utils.TanhNormalFactory","title":"<code>src.jaxnasium.algorithms.utils.TanhNormalFactory(low, high) -&gt; Callable[..., TanhNormal]</code>","text":""},{"location":"algorithms/misc/Initialization/","title":"Initialization","text":""},{"location":"algorithms/misc/Initialization/#src.jaxnasium.algorithms.utils.rl_initialization","title":"<code>src.jaxnasium.algorithms.utils.rl_initialization(key: PRNGKeyArray, network: eqx.Module, weight_init: jax.nn.initializers.Initializer = jax.nn.initializers.orthogonal(), bias_init=0.0)</code>","text":"<p>Sets all layers in a network to a given weight and bias initialization. Defaults to orthogonal weight initialization and zero bias initialization.</p>"},{"location":"algorithms/misc/Logging/","title":"Logging","text":""},{"location":"algorithms/misc/Logging/#src.jaxnasium.algorithms.utils.scan_callback","title":"<code>src.jaxnasium.algorithms.utils.scan_callback(func: Optional[Callable] = None, callback_fn: Optional[Callable | Literal['tqdm', 'simple']] = None, callback_interval: int | float = 20, n: Optional[int] = None) -&gt; Callable</code>","text":""},{"location":"algorithms/misc/Logging/#src.jaxnasium.algorithms.utils.pretty_print_network","title":"<code>src.jaxnasium.algorithms.utils.pretty_print_network(network: eqx.Module)</code>","text":""},{"location":"algorithms/misc/Normalization/","title":"Normalization","text":""},{"location":"algorithms/misc/Normalization/#src.jaxnasium.algorithms.utils.Normalizer","title":"<code>src.jaxnasium.algorithms.utils.Normalizer</code>","text":"<p>A container for running statistics on Observations and Rewards.</p>"},{"location":"algorithms/misc/Normalization/#src.jaxnasium.algorithms.utils.Normalizer.normalize_obs","title":"<code>normalize_obs(obs: PyTree) -&gt; PyTree</code>","text":"<p>Normalizes the given batch of observations if normalization of observations is enabled.</p>"},{"location":"algorithms/misc/Normalization/#src.jaxnasium.algorithms.utils.Normalizer.normalize_reward","title":"<code>normalize_reward(reward: Array) -&gt; Array</code>","text":"<p>Normalizes the given batch of rewards if normalization of rewards is enabled.</p>"},{"location":"algorithms/misc/Normalization/#src.jaxnasium.algorithms.utils.Normalizer.update","title":"<code>update(batch: Transition) -&gt; Normalizer</code>","text":"<p>Updates the normalization state with a new Transition containing both rewards and observations.</p>"},{"location":"algorithms/misc/Replay-Buffer/","title":"Replay Buffer","text":""},{"location":"algorithms/misc/Replay-Buffer/#src.jaxnasium.algorithms.utils.TransitionBuffer","title":"<code>src.jaxnasium.algorithms.utils.TransitionBuffer</code>","text":"<p>A buffer for storing transitions. Samples uniformly from the buffer. The buffer is implemented as a circular buffer, where the oldest transitions are overwritten when the buffer is full.</p> <p>Arguments:     <code>max_size</code>: The maximum size of the buffer.     <code>sample_batch_size</code>: The number of transitions to sample from the buffer.     <code>data_sample</code>: A sample <code>Transition</code> to initialize the buffer structure.     <code>num_batch_axes</code>: The number of batch axes in the transition data. Defaults to 2, which expects the first two axes         to be batch dimensions (e.g, (<code>rollout_length</code>, <code>num_envs</code>, ...))</p>"},{"location":"algorithms/misc/Replay-Buffer/#src.jaxnasium.algorithms.utils.TransitionBuffer.insert","title":"<code>insert(transition: Transition) -&gt; TransitionBuffer</code>","text":"<p>Insert a transition into the buffer.</p>"},{"location":"algorithms/misc/Replay-Buffer/#src.jaxnasium.algorithms.utils.TransitionBuffer.sample","title":"<code>sample(key: PRNGKeyArray, with_replacement: bool = False) -&gt; Transition</code>","text":"<p>Sample a batch of transitions from the buffer.</p>"},{"location":"algorithms/misc/Transitions/","title":"Transitions","text":""},{"location":"algorithms/misc/Transitions/#src.jaxnasium.algorithms.utils.Transition","title":"<code>src.jaxnasium.algorithms.utils.Transition</code>","text":"<p>Container for (possibly batches of) transitions Comes with functionality of creating minibatches and some utilities for multi-agent reinforcement learning.</p>"},{"location":"algorithms/misc/Transitions/#src.jaxnasium.algorithms.utils.Transition.make_minibatches","title":"<code>make_minibatches(key: PRNGKeyArray, n_minibatches: int, n_epochs: int = 1, n_batch_axis: int = 1) -&gt; Transition</code>","text":"<p>Creates shuffled minibatches from the transition. Returns a copy of the transition with each leaf reshaped to (num_minibatches, ...),</p> <p>This function first flattens the transition over the leading n_batch_axis. This is useful if your data hasn't been flattened yet and may be structured as (rollout_length, num_envs, ...), where num_envs is the number of parallel environments.</p> <p>If n_epochs &gt; 1, it will create n_epochs copies of the minibatches. and stack these such that there is a single leading axis to scan over for training.</p> <p>If the batch size is not divisible by the number of minibatches, the remainder is truncated so that each minibatch has the same size.</p> <p>Arguments: - <code>key</code>: JAX PRNG key for randomization. - <code>num_minibatches</code>: Number of minibatches to create. - <code>n_epochs</code>: Number of copies the minibatches should be stacked. - <code>n_batch_axis</code>: Number of leading batch axes to flatten over. Default is 1 (already flattened).</p>"},{"location":"algorithms/misc/Transitions/#src.jaxnasium.algorithms.utils.Transition.view_transposed","title":"<code>view_transposed: PyTree[Transition]</code>  <code>property</code>","text":"<p>For single-agent settings, this will do nothing and return the original transition.</p> <p>For multi-agent settings: The original transition is a Transition of PyTrees     e.g. Transition(observation={a1: ..., a2: ...}, action={a1: ..., a2: ...}, ...) The transposed transition is a PyTree of Transitions     e.g. {a1: Transition(observation=..., action=..., ...), a2: Transition(observation=..., action=..., ...), ...} This is useful for multi-agent environments where we want to have a single Transition object per agent.</p>"},{"location":"algorithms/misc/Transitions/#src.jaxnasium.algorithms.utils.Transition.structure","title":"<code>structure: PyTreeDef</code>  <code>property</code>","text":"<p>Returns the top-level structure of the transition objects (using reward as a reference). This is either PyTreeDef() for single agents or PyTreeDef((, x num_agents)) for multi-agent environments. usefull for unflattening Transition.flat.properties back to the original structure.</p>"},{"location":"algorithms/misc/Transitions/#src.jaxnasium.algorithms.utils.Transition.view_flat","title":"<code>view_flat: Transition</code>  <code>property</code>","text":"<p>Returns a flattened version of the transition. Where possible, this is a jnp.stack of the leaves. Otherwise, it returns a list of leaves.</p>"},{"location":"algorithms/misc/Transitions/#src.jaxnasium.algorithms.utils.Transition.from_transposed","title":"<code>from_transposed(transposed: PyTree[Transition]) -&gt; Transition</code>  <code>classmethod</code>","text":""},{"location":"api/Available-Environments/","title":"Available Environments","text":"<p>Jaxnasium doesn't bundle a large number of environments directly. Instead, it relies on existing work and wraps these environments in various wrappers to conform to the Jaxnasium API. This approach allows users to leverage a wide array of established environments while maintaining the performance and flexibility offered by JAX.</p>"},{"location":"api/Available-Environments/#native-environments","title":"Native Environments","text":"<p>Note: The classic control environments are natively implemented \u00e1nd bundled in Jaxnasium convenience:</p> <ul> <li><code>CartPole-v1</code></li> <li><code>MountainCar-v0</code> </li> <li><code>Acrobot-v1</code></li> <li><code>Pendulum-v1</code></li> <li><code>MountainCarContinuous-v0</code></li> </ul> <p>These environments are implemented directly in Jaxnasium and don't require external dependencies.</p>"},{"location":"api/Available-Environments/#external-environments-using-the-jaxnasium-api","title":"External Environments using the Jaxnasium API","text":"<p>These environments run without wrappers.</p>"},{"location":"api/Available-Environments/#external-environment-libraries","title":"External Environment Libraries","text":"<p>Jaxnasium integrates with the following external environment libraries through wrapper adapters. See the end of this page for a full list of available environments.</p>"},{"location":"api/Available-Environments/#gymnax","title":"Gymnax","text":"<p>JAX implementations of OpenAI's Gym environments, offering accelerated and parallelized rollouts. Includes classic control, bsuite, and MinAtar environments.</p>"},{"location":"api/Available-Environments/#jumanji","title":"Jumanji","text":"<p>A suite of diverse, scalable reinforcement learning environments implemented in JAX by DeepMind. Focuses on combinatorial problems and general decision-making tasks.</p>"},{"location":"api/Available-Environments/#brax","title":"Brax","text":"<p>A fast and flexible physics simulation engine for training and evaluating rigid body environments in JAX by Google.</p>"},{"location":"api/Available-Environments/#pgx","title":"Pgx","text":"<p>JAX implementations of various board games and classic environments, including chess, Go, shogi, and more.</p>"},{"location":"api/Available-Environments/#jaxmarl","title":"JaxMARL","text":"<p>Multi-agent reinforcement learning environments implemented in JAX, including MPE (Multi-Particle Environment) scenarios and other multi-agent tasks.</p>"},{"location":"api/Available-Environments/#xminigrid","title":"xMinigrid","text":"<p>JAX implementation of MiniGrid environments, including XLand variants for procedural generation research.</p>"},{"location":"api/Available-Environments/#navix","title":"Navix","text":"<p>JAX implementation of navigation environments, providing various gridworld navigation tasks.</p>"},{"location":"api/Available-Environments/#craftax","title":"Craftax","text":"<p>JAX implementation of Craftax environments, inspired by Minecraft-like crafting and survival tasks.</p>"},{"location":"api/Available-Environments/#usage","title":"Usage","text":"<p>To use any of these environments, simply call:</p> <pre><code>import jaxnasium as jym\n\n# Native environments\nenv = jym.make(\"CartPole-v1\")\n\n# External environments (requires installing the respective library)\nenv = jym.make(\"Breakout-MinAtar\")  # Gymnax\nenv = jym.make(\"Game2048-v1\")       # Jumanji\nenv = jym.make(\"ant\")               # Brax\nenv = jym.make(\"chess\")             # Pgx\n</code></pre> <p>Note: External environment libraries are not bundled as dependencies and need to be installed manually (e.g., via pip) before use.</p>"},{"location":"api/Available-Environments/#complete-list-of-registered-environments","title":"Complete List of Registered Environments","text":"<p>Below is the complete list of all registered environment strings available in Jaxnasium:</p> <p>Auto-generated List</p> <p>This list is automatically generated from the Jaxnasium registry.</p> <ul> <li><code>Acrobot-v1</code></li> <li><code>CartPole-v1</code></li> <li><code>MountainCarContinuous-v0</code></li> <li><code>MountainCar-v0</code></li> <li><code>Pendulum-v1</code></li> <li><code>gymnax:CartPole-v1</code></li> <li><code>gymnax:Acrobot-v1</code></li> <li><code>gymnax:Pendulum-v1</code></li> <li><code>gymnax:MountainCar-v0</code></li> <li><code>gymnax:MountainCarContinuous-v0</code></li> <li><code>gymnax:Asterix-MinAtar</code></li> <li><code>gymnax:Breakout-MinAtar</code></li> <li><code>gymnax:Freeway-MinAtar</code></li> <li><code>gymnax:SpaceInvaders-MinAtar</code></li> <li><code>gymnax:DeepSea-bsuite</code></li> <li><code>gymnax:Catch-bsuite</code></li> <li><code>gymnax:MemoryChain-bsuite</code></li> <li><code>gymnax:UmbrellaChain-bsuite</code></li> <li><code>gymnax:DiscountingChain-bsuite</code></li> <li><code>gymnax:MNISTBandit-bsuite</code></li> <li><code>gymnax:SimpleBandit-bsuite</code></li> <li><code>gymnax:FourRooms-misc</code></li> <li><code>gymnax:MetaMaze-misc</code></li> <li><code>gymnax:PointRobot-misc</code></li> <li><code>gymnax:BernoulliBandit-misc</code></li> <li><code>gymnax:GaussianBandit-misc</code></li> <li><code>gymnax:Reacher-misc</code></li> <li><code>gymnax:Swimmer-misc</code></li> <li><code>gymnax:Pong-misc</code></li> <li><code>jumanji:Game2048-v1</code></li> <li><code>jumanji:GraphColoring-v1</code></li> <li><code>jumanji:Minesweeper-v0</code></li> <li><code>jumanji:RubiksCube-v0</code></li> <li><code>jumanji:RubiksCube-partly-scrambled-v0</code></li> <li><code>jumanji:SlidingTilePuzzle-v0</code></li> <li><code>jumanji:Sudoku-v0</code></li> <li><code>jumanji:Sudoku-very-easy-v0</code></li> <li><code>jumanji:BinPack-v2</code></li> <li><code>jumanji:FlatPack-v0</code></li> <li><code>jumanji:JobShop-v0</code></li> <li><code>jumanji:Knapsack-v1</code></li> <li><code>jumanji:Tetris-v0</code></li> <li><code>jumanji:Cleaner-v0</code></li> <li><code>jumanji:Connector-v2</code></li> <li><code>jumanji:CVRP-v1</code></li> <li><code>jumanji:MultiCVRP-v0</code></li> <li><code>jumanji:Maze-v0</code></li> <li><code>jumanji:RobotWarehouse-v0</code></li> <li><code>jumanji:Snake-v1</code></li> <li><code>jumanji:TSP-v1</code></li> <li><code>jumanji:MMST-v0</code></li> <li><code>jumanji:PacMan-v1</code></li> <li><code>jumanji:Sokoban-v0</code></li> <li><code>jumanji:LevelBasedForaging-v0</code></li> <li><code>jumanji:SearchAndRescue-v0</code></li> <li><code>brax:ant</code></li> <li><code>brax:halfcheetah</code></li> <li><code>brax:hopper</code></li> <li><code>brax:humanoid</code></li> <li><code>brax:humanoidstandup</code></li> <li><code>brax:inverted_pendulum</code></li> <li><code>brax:inverted_double_pendulum</code></li> <li><code>brax:pusher</code></li> <li><code>brax:reacher</code></li> <li><code>brax:walker2d</code></li> <li><code>pgx:2048</code></li> <li><code>pgx:animal_shogi</code></li> <li><code>pgx:backgammon</code></li> <li><code>pgx:chess</code></li> <li><code>pgx:connect_four</code></li> <li><code>pgx:gardner_chess</code></li> <li><code>pgx:go_9x9</code></li> <li><code>pgx:go_19x19</code></li> <li><code>pgx:hex</code></li> <li><code>pgx:kuhn_poker</code></li> <li><code>pgx:leduc_holdem</code></li> <li><code>pgx:minatar-asterix</code></li> <li><code>pgx:minatar-breakout</code></li> <li><code>pgx:minatar-freeway</code></li> <li><code>pgx:minatar-seaquest</code></li> <li><code>pgx:minatar-space_invaders</code></li> <li><code>pgx:othello</code></li> <li><code>pgx:shogi</code></li> <li><code>pgx:sparrow_mahjong</code></li> <li><code>pgx:tic_tac_toe</code></li> <li><code>jaxmarl:MPE_simple_v3</code></li> <li><code>jaxmarl:MPE_simple_tag_v3</code></li> <li><code>jaxmarl:MPE_simple_world_comm_v3</code></li> <li><code>jaxmarl:MPE_simple_spread_v3</code></li> <li><code>jaxmarl:MPE_simple_crypto_v3</code></li> <li><code>jaxmarl:MPE_simple_speaker_listener_v4</code></li> <li><code>jaxmarl:MPE_simple_push_v3</code></li> <li><code>jaxmarl:MPE_simple_adversary_v3</code></li> <li><code>jaxmarl:MPE_simple_reference_v3</code></li> <li><code>jaxmarl:MPE_simple_facmac_v1</code></li> <li><code>jaxmarl:MPE_simple_facmac_3a_v1</code></li> <li><code>jaxmarl:MPE_simple_facmac_6a_v1</code></li> <li><code>jaxmarl:MPE_simple_facmac_9a_v1</code></li> <li><code>jaxmarl:switch_riddle</code></li> <li><code>jaxmarl:SMAX</code></li> <li><code>jaxmarl:HeuristicEnemySMAX</code></li> <li><code>jaxmarl:ant_4x2</code></li> <li><code>jaxmarl:halfcheetah_6x1</code></li> <li><code>jaxmarl:hopper_3x1</code></li> <li><code>jaxmarl:humanoid_9|8</code></li> <li><code>jaxmarl:walker2d_2x3</code></li> <li><code>jaxmarl:storm</code></li> <li><code>jaxmarl:storm_2p</code></li> <li><code>jaxmarl:storm_np</code></li> <li><code>jaxmarl:hanabi</code></li> <li><code>jaxmarl:overcooked</code></li> <li><code>jaxmarl:overcooked_v2</code></li> <li><code>jaxmarl:coin_game</code></li> <li><code>jaxmarl:jaxnav</code></li> <li><code>xminigrid:XLand-MiniGrid-R1-9x9</code></li> <li><code>xminigrid:XLand-MiniGrid-R1-11x11</code></li> <li><code>xminigrid:XLand-MiniGrid-R1-13x13</code></li> <li><code>xminigrid:XLand-MiniGrid-R1-15x15</code></li> <li><code>xminigrid:XLand-MiniGrid-R1-17x17</code></li> <li><code>xminigrid:XLand-MiniGrid-R2-9x9</code></li> <li><code>xminigrid:XLand-MiniGrid-R2-11x11</code></li> <li><code>xminigrid:XLand-MiniGrid-R2-13x13</code></li> <li><code>xminigrid:XLand-MiniGrid-R2-15x15</code></li> <li><code>xminigrid:XLand-MiniGrid-R2-17x17</code></li> <li><code>xminigrid:XLand-MiniGrid-R4-9x9</code></li> <li><code>xminigrid:XLand-MiniGrid-R4-11x11</code></li> <li><code>xminigrid:XLand-MiniGrid-R4-13x13</code></li> <li><code>xminigrid:XLand-MiniGrid-R4-15x15</code></li> <li><code>xminigrid:XLand-MiniGrid-R4-17x17</code></li> <li><code>xminigrid:XLand-MiniGrid-R6-13x13</code></li> <li><code>xminigrid:XLand-MiniGrid-R6-17x17</code></li> <li><code>xminigrid:XLand-MiniGrid-R6-19x19</code></li> <li><code>xminigrid:XLand-MiniGrid-R9-16x16</code></li> <li><code>xminigrid:XLand-MiniGrid-R9-19x19</code></li> <li><code>xminigrid:XLand-MiniGrid-R9-25x25</code></li> <li><code>xminigrid:MiniGrid-BlockedUnlockPickUp</code></li> <li><code>xminigrid:MiniGrid-DoorKey-5x5</code></li> <li><code>xminigrid:MiniGrid-DoorKey-6x6</code></li> <li><code>xminigrid:MiniGrid-DoorKey-8x8</code></li> <li><code>xminigrid:MiniGrid-DoorKey-16x16</code></li> <li><code>xminigrid:MiniGrid-Empty-5x5</code></li> <li><code>xminigrid:MiniGrid-Empty-6x6</code></li> <li><code>xminigrid:MiniGrid-Empty-8x8</code></li> <li><code>xminigrid:MiniGrid-Empty-16x16</code></li> <li><code>xminigrid:MiniGrid-EmptyRandom-5x5</code></li> <li><code>xminigrid:MiniGrid-EmptyRandom-6x6</code></li> <li><code>xminigrid:MiniGrid-EmptyRandom-8x8</code></li> <li><code>xminigrid:MiniGrid-EmptyRandom-16x16</code></li> <li><code>xminigrid:MiniGrid-FourRooms</code></li> <li><code>xminigrid:MiniGrid-LockedRoom</code></li> <li><code>xminigrid:MiniGrid-MemoryS8</code></li> <li><code>xminigrid:MiniGrid-MemoryS16</code></li> <li><code>xminigrid:MiniGrid-MemoryS32</code></li> <li><code>xminigrid:MiniGrid-MemoryS64</code></li> <li><code>xminigrid:MiniGrid-MemoryS128</code></li> <li><code>xminigrid:MiniGrid-Playground</code></li> <li><code>xminigrid:MiniGrid-Unlock</code></li> <li><code>xminigrid:MiniGrid-UnlockPickUp</code></li> <li><code>navix:Navix-Empty-5x5-v0</code></li> <li><code>navix:Navix-Empty-6x6-v0</code></li> <li><code>navix:Navix-Empty-8x8-v0</code></li> <li><code>navix:Navix-Empty-16x16-v0</code></li> <li><code>navix:Navix-Empty-Random-5x5-v0</code></li> <li><code>navix:Navix-Empty-Random-6x6-v0</code></li> <li><code>navix:Navix-Empty-Random-8x8-v0</code></li> <li><code>navix:Navix-Empty-Random-16x16-v0</code></li> <li><code>navix:Navix-DoorKey-5x5-v0</code></li> <li><code>navix:Navix-DoorKey-6x6-v0</code></li> <li><code>navix:Navix-DoorKey-8x8-v0</code></li> <li><code>navix:Navix-DoorKey-16x16-v0</code></li> <li><code>navix:Navix-DoorKey-Random-5x5-v0</code></li> <li><code>navix:Navix-DoorKey-Random-6x6-v0</code></li> <li><code>navix:Navix-DoorKey-Random-8x8-v0</code></li> <li><code>navix:Navix-DoorKey-Random-16x16-v0</code></li> <li><code>navix:Navix-FourRooms-v0</code></li> <li><code>navix:Navix-KeyCorridorS3R1-v0</code></li> <li><code>navix:Navix-KeyCorridorS3R2-v0</code></li> <li><code>navix:Navix-KeyCorridorS3R3-v0</code></li> <li><code>navix:Navix-KeyCorridorS4R3-v0</code></li> <li><code>navix:Navix-KeyCorridorS5R3-v0</code></li> <li><code>navix:Navix-KeyCorridorS6R3-v0</code></li> <li><code>navix:Navix-LavaGapS5-v0</code></li> <li><code>navix:Navix-LavaGapS6-v0</code></li> <li><code>navix:Navix-LavaGapS7-v0</code></li> <li><code>navix:Navix-SimpleCrossingS9N1-v0</code></li> <li><code>navix:Navix-SimpleCrossingS9N2-v0</code></li> <li><code>navix:Navix-SimpleCrossingS9N3-v0</code></li> <li><code>navix:Navix-SimpleCrossingS11N5-v0</code></li> <li><code>navix:Navix-Dynamic-Obstacles-5x5-v0</code></li> <li><code>navix:Navix-Dynamic-Obstacles-5x5-Random-v0</code></li> <li><code>navix:Navix-Dynamic-Obstacles-6x6-v0</code></li> <li><code>navix:Navix-Dynamic-Obstacles-6x6-Random-v0</code></li> <li><code>navix:Navix-Dynamic-Obstacles-8x8-v0</code></li> <li><code>navix:Navix-Dynamic-Obstacles-16x16-v0</code></li> <li><code>navix:Navix-DistShift1-v0</code></li> <li><code>navix:Navix-DistShift2-v0</code></li> <li><code>navix:Navix-GoToDoor-5x5-v0</code></li> <li><code>navix:Navix-GoToDoor-6x6-v0</code></li> <li><code>navix:Navix-GoToDoor-8x8-v0</code></li> <li><code>craftax:Craftax-Classic-Symbolic-v1</code></li> <li><code>craftax:Craftax-Classic-Pixels-v1</code></li> <li><code>craftax:Craftax-Symbolic-v1</code></li> <li><code>craftax:Craftax-Pixels-v1</code></li> </ul>"},{"location":"api/Environment/","title":"Environment","text":""},{"location":"api/Environment/#src.jaxnasium.Environment","title":"<code>src.jaxnasium.Environment</code>","text":"<p>Base environment class for JAX-compatible environments. Create your environment by subclassing this.</p> <p><code>step</code> and <code>reset</code> should typically not be overridden, as they merely handle the auto-reset logic. Instead, the environment-specific logic should be implemented in the <code>step_env</code> and <code>reset_env</code> methods.</p>"},{"location":"api/Environment/#src.jaxnasium.Environment.step","title":"<code>step(key: PRNGKeyArray, state: TEnvState, action: PyTree[Real[Array, ...]]) -&gt; Tuple[TimeStep, TEnvState]</code>","text":"<p>Steps the environment forward with the given action and performs auto-reset when necessary. Additionally, this function inserts the original observation (before auto-resetting) in the info dictionary to bootstrap correctly on truncated episodes (<code>info={\"_TERMINAL_OBSERVATION\": obs, ...}</code>)</p> <p>This function should typically not be overridden. Instead, the environment-specific logic should be implemented in the <code>step_env</code> method.</p> <p>Returns a TimeStep object (observation, reward, terminated, truncated, info) and the new state.</p> <p>Arguments:</p> <ul> <li><code>key</code>: JAX PRNG key.</li> <li><code>state</code>: Current state of the environment.</li> <li><code>action</code>: Action to take in the environment.</li> </ul>"},{"location":"api/Environment/#src.jaxnasium.Environment.step_env","title":"<code>step_env(key: PRNGKeyArray, state: TEnvState, action: PyTree[Real[Array, ...]]) -&gt; Tuple[TimeStep, TEnvState]</code>  <code>abstractmethod</code>","text":"<p>Defines the environment-specific step logic. I.e. here the state of the environment is updated according to the transition function.</p> <p>Returns a <code>TimeStep</code> object (observation, reward, terminated, truncated, info) and the new state.</p> <p>Arguments:</p> <ul> <li><code>key</code>: JAX PRNG key.</li> <li><code>state</code>: Current state of the environment.</li> <li><code>action</code>: Action to take in the environment.</li> </ul>"},{"location":"api/Environment/#src.jaxnasium.Environment.reset","title":"<code>reset(key: PRNGKeyArray) -&gt; Tuple[TObservation, TEnvState]</code>","text":"<p>Resets the environment to an initial state and returns the initial observation. Environment-specific logic is defined in the <code>reset_env</code> method. Typically, this function should not be overridden.</p> <p>Returns the initial observation and the initial state of the environment.</p> <p>Arguments:</p> <ul> <li><code>key</code>: JAX PRNG key.</li> </ul>"},{"location":"api/Environment/#src.jaxnasium.Environment.reset_env","title":"<code>reset_env(key: PRNGKeyArray) -&gt; Tuple[TObservation, TEnvState]</code>  <code>abstractmethod</code>","text":"<p>Defines the environment-specific reset logic.</p> <p>Returns the initial observation and the initial state of the environment.</p> <p>Arguments:</p> <ul> <li><code>key</code>: JAX PRNG key.</li> </ul>"},{"location":"api/Environment/#src.jaxnasium.Environment.observation_space","title":"<code>observation_space: Space | PyTree[Space]</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Defines the space of possible observations from the environment. For multi-agent environments, this should be a PyTree of spaces. See <code>jaxnasium.spaces</code> for more information on how to define (composite) observation spaces.</p>"},{"location":"api/Environment/#src.jaxnasium.Environment.sample_observation","title":"<code>sample_observation(key: PRNGKeyArray) -&gt; TObservation</code>","text":"<p>Convenience method to sample a random observation from the environment's observation space. While one could use <code>self.observation_space.sample(key)</code>, this method additionally works on composite observation spaces.</p>"},{"location":"api/Environment/#src.jaxnasium.Environment.action_space","title":"<code>action_space: Space | PyTree[Space]</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Defines the space of valid actions for the environment. For multi-agent environments, this should be a PyTree of spaces. See <code>jaxnasium.spaces</code> for more information on how to define (composite) action spaces.</p>"},{"location":"api/Environment/#src.jaxnasium.Environment.sample_action","title":"<code>sample_action(key: PRNGKeyArray) -&gt; PyTree[Real[Array, ...]]</code>","text":"<p>Convenience method to sample a random action from the environment's action space. While one could use <code>self.action_space.sample(key)</code>, this method additionally works on composite action spaces.</p>"},{"location":"api/Environment/#src.jaxnasium.Environment.auto_reset","title":"<code>auto_reset(key: PRNGKeyArray, timestep_step: TimeStep, state_step: TEnvState) -&gt; Tuple[TimeStep, TEnvState]</code>","text":"<p>Auto-resets the environment when the episode is terminated or truncated.</p> <p>Given a step timestep and state, this function will auto-reset the environment and return the new timestep and state when the episode is terminated or truncated. Inserts the original observation in info to bootstrap correctly on truncated episodes.</p> <p>Arguments:</p> <ul> <li><code>key</code>: JAX PRNG key.</li> <li><code>timestep_step</code>: The timestep returned by the <code>step_env</code> method.</li> <li><code>state_step</code>: The state returned by the <code>step_env</code> method.</li> </ul> <p>Returns: A tuple of the new timestep and state with the state and observation reset to a new initial state and observation when the episode is terminated or truncated. The original observation is inserted in info to bootstrap correctly on truncated episodes.</p>"},{"location":"api/Environment/#src.jaxnasium.Environment.multi_agent","title":"<code>multi_agent: bool</code>  <code>property</code>","text":"<p>Indicates if the environment is a multi-agent environment.</p> <p>Infers this via the <code>_multi_agent</code> property. If not set, assumes single-agent.</p>"},{"location":"api/Environment/#src.jaxnasium.Environment.agent_structure","title":"<code>agent_structure: PyTreeDef</code>  <code>property</code>","text":"<p>Returns the structure of the agent space. In single-agent environments this is simply a PyTreeDef(). However, for multi-agent environments this is a PyTreeDef((, x num_agents)).</p>"},{"location":"api/Environment/#timestep","title":"Timestep","text":""},{"location":"api/Environment/#src.jaxnasium.TimeStep","title":"<code>src.jaxnasium.TimeStep</code>","text":"<p>A container for the output of an environment's step function. (<code>timestep, state = env.step(...)</code>).</p> <p>This class follows the Gymnasium standard API, with the signature: <code>(obs, reward, terminated, truncated, info)</code> tuple.</p> <p>Arguments:</p> <ul> <li><code>observation</code>: The environment state representation provided to the agent.   Can be an Array or a PyTree of arrays.   When using action masking, the observation should be of type <code>AgentObservation</code>.</li> <li><code>reward</code>: The reward signal from the previous action, indicating performance.   Can be a scalar Array or a PyTree of reward Arrays (in the case of multi agent-environments).</li> <li><code>terminated</code>: Boolean flag indicating whether the episode has ended due to reaching a terminal state (e.g., goal reached, game over).</li> <li><code>truncated</code>: Boolean flag indicating whether the episode ended due to external factors (e.g., reaching max steps, timeout).</li> <li><code>info</code>: Dictionary containing any additional information about the environment step.</li> </ul>"},{"location":"api/Environment/#observation-container-optional","title":"Observation container (optional)","text":""},{"location":"api/Environment/#src.jaxnasium.AgentObservation","title":"<code>src.jaxnasium.AgentObservation</code>","text":"<p>A container for the observation of a single agent, with optional action masking.</p> <p>Typically, this container is optional. However, Algorithms in <code>jaxnasium.algorithms</code> expect observations to be wrapped in this type when action masking is enabled.</p> <p>Arguments:</p> <ul> <li><code>observation</code>: The observation of the agent.</li> <li><code>action_mask</code>: The action mask of the agent. A boolean array of the same shape as the action space.</li> </ul>"},{"location":"api/Spaces/","title":"Spaces","text":""},{"location":"api/Spaces/#src.jaxnasium._wrappers.Space","title":"<code>src.jaxnasium._wrappers.Space</code>","text":"<p>The base class for all spaces in Jaxnasium. Instead of using this class directly, use the subclasses <code>Box</code>, <code>Discrete</code>, and <code>MultiDiscrete</code>. Composite spaces can be created by simply combining spaces in an arbitrary PyTree. For example, a tuple of Box spaces can be created as follows: <pre><code>from jaxnasium import Box\n\nbox1 = Box(low=0, high=1, shape=(3,))\nbox2 = Box(low=0, high=1, shape=(4,))\nbox3 = Box(low=0, high=1, shape=(5,))\ncomposite_space = (box1, box2, box3)\n</code></pre></p> <p>Jaxnasium algorithms assume multi-agent environments are such a composite space, where the first level of the PyTree is the agent dimension. For example, a multi-agent environment observation space may look like this: <pre><code>from jaxnasium import Box\nfrom jaxnasium import MultiDiscrete\n\nagent1_obs = Box(low=0, high=1, shape=(3,))\nagent2_obs = Box(low=0, high=1, shape=(4,))\nagent3_obs = MultiDiscrete(nvec=[2, 3])\nenv_obs_space = {\n    \"agent1\": agent1_obs,\n    \"agent2\": agent2_obs,\n    \"agent3\": agent3_obs,\n}\n</code></pre></p> <p>Spaces are purposefully not registered PyTree nodes.</p>"},{"location":"api/Spaces/#src.jaxnasium._wrappers.Space.shape","title":"<code>shape: eqx.AbstractVar[tuple[int, ...]]</code>  <code>instance-attribute</code>","text":""},{"location":"api/Spaces/#src.jaxnasium._wrappers.Space.sample","title":"<code>sample(rng: PRNGKeyArray) -&gt; Array</code>  <code>abstractmethod</code>","text":""},{"location":"api/Spaces/#src.jaxnasium.Box","title":"<code>src.jaxnasium.Box</code>  <code>dataclass</code>","text":"<p>The standard Box space for continuous action/observation spaces.</p> <p>Arguments:</p> <ul> <li><code>low</code> (int / Array[int]): The lower bound of the space.</li> <li><code>high</code> (int / Array[int]): The upper bound of the space.</li> <li><code>shape</code>: The shape of the space.</li> <li><code>dtype</code>: The data type of the space. Default is jnp.float32.</li> </ul>"},{"location":"api/Spaces/#src.jaxnasium.Box.sample","title":"<code>sample(rng: PRNGKeyArray) -&gt; Array</code>","text":"<p>Sample random action uniformly from set of continuous choices.</p>"},{"location":"api/Spaces/#src.jaxnasium.Discrete","title":"<code>src.jaxnasium.Discrete</code>  <code>dataclass</code>","text":"<p>The standard discrete space for discrete action/observation spaces.</p> <p>Arguments:</p> <ul> <li><code>n</code> (int): The number of discrete actions.</li> <li><code>dtype</code>: The data type of the space. Default is jnp.int16.</li> </ul>"},{"location":"api/Spaces/#src.jaxnasium.Discrete.sample","title":"<code>sample(rng: PRNGKeyArray) -&gt; Int[Array, '']</code>","text":"<p>Sample random action uniformly from set of discrete choices.</p>"},{"location":"api/Spaces/#src.jaxnasium.MultiDiscrete","title":"<code>src.jaxnasium.MultiDiscrete</code>  <code>dataclass</code>","text":"<p>The standard multi-discrete space for discrete action/observation spaces.</p> <p>Arguments:</p> <ul> <li><code>nvec</code> (Array[int]): The number of discrete actions for each dimension.</li> <li><code>dtype</code>: The data type of the space. Default is jnp.int16.</li> </ul>"},{"location":"api/Spaces/#src.jaxnasium.MultiDiscrete.sample","title":"<code>sample(rng: PRNGKeyArray) -&gt; Int[Array, '']</code>","text":"<p>Sample random action uniformly from set of discrete choices.</p>"},{"location":"api/Wrappers/","title":"Wrappers","text":""},{"location":"api/Wrappers/#src.jaxnasium.LogWrapper","title":"<code>src.jaxnasium.LogWrapper</code>","text":"<p>Log the episode returns and lengths. Modeled after the LogWrapper in PureJaxRL.</p> <p>This wrapper inserts episode returns and lengths into the <code>info</code> dictionary of the <code>TimeStep</code> object. The <code>returned_episode_returns</code> and <code>returned_episode_lengths</code> are the returns and lengths of the last completed episode.</p> <p>After collecting a trajectory of <code>n</code> steps and collecting all the info dicts, the episode returns may be collected via: <pre><code>return_values = jax.tree.map(\n    lambda x: x[data[\"returned_episode\"]], data[\"returned_episode_returns\"]\n)\n</code></pre></p> <p>Arguments:</p> <ul> <li><code>_env</code>: Environment to wrap.</li> </ul>"},{"location":"api/Wrappers/#src.jaxnasium.LogWrapper.step_env","title":"<code>step_env(key: PRNGKeyArray, state: TEnvState, action: PyTree[Real[Array, ...]]) -&gt; Tuple[TimeStep, TEnvState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.LogWrapper.reset_env","title":"<code>reset_env(key: PRNGKeyArray) -&gt; Tuple[TObservation, TEnvState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.LogWrapper.action_space","title":"<code>action_space: Space | PyTree[Space]</code>  <code>property</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.LogWrapper.observation_space","title":"<code>observation_space: Space | PyTree[Space]</code>  <code>property</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.LogWrapper.auto_reset","title":"<code>auto_reset(key: PRNGKeyArray, timestep_step: TimeStep, state_step: TEnvState) -&gt; Tuple[TimeStep, TEnvState]</code>","text":"<p>Auto-resets the environment when the episode is terminated or truncated.</p> <p>Given a step timestep and state, this function will auto-reset the environment and return the new timestep and state when the episode is terminated or truncated. Inserts the original observation in info to bootstrap correctly on truncated episodes.</p> <p>Arguments:</p> <ul> <li><code>key</code>: JAX PRNG key.</li> <li><code>timestep_step</code>: The timestep returned by the <code>step_env</code> method.</li> <li><code>state_step</code>: The state returned by the <code>step_env</code> method.</li> </ul> <p>Returns: A tuple of the new timestep and state with the state and observation reset to a new initial state and observation when the episode is terminated or truncated. The original observation is inserted in info to bootstrap correctly on truncated episodes.</p>"},{"location":"api/Wrappers/#src.jaxnasium.LogWrapper.sample_action","title":"<code>sample_action(key: PRNGKeyArray) -&gt; PyTree[Real[Array, ...]]</code>","text":"<p>Convenience method to sample a random action from the environment's action space. While one could use <code>self.action_space.sample(key)</code>, this method additionally works on composite action spaces.</p>"},{"location":"api/Wrappers/#src.jaxnasium.LogWrapper.sample_observation","title":"<code>sample_observation(key: PRNGKeyArray) -&gt; TObservation</code>","text":"<p>Convenience method to sample a random observation from the environment's observation space. While one could use <code>self.observation_space.sample(key)</code>, this method additionally works on composite observation spaces.</p>"},{"location":"api/Wrappers/#src.jaxnasium.LogWrapper.multi_agent","title":"<code>multi_agent: bool</code>  <code>property</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.LogWrapper.agent_structure","title":"<code>agent_structure: PyTreeDef</code>  <code>property</code>","text":"<p>Returns the structure of the agent space. In single-agent environments this is simply a PyTreeDef(). However, for multi-agent environments this is a PyTreeDef((, x num_agents)).</p>"},{"location":"api/Wrappers/#src.jaxnasium.LogWrapper.__getattr__","title":"<code>__getattr__(name)</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.LogWrapper.reset","title":"<code>reset(key: PRNGKeyArray) -&gt; Tuple[TObservation, LogEnvState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.LogWrapper.step","title":"<code>step(key: PRNGKeyArray, state: LogEnvState, action: PyTree[int | float | Array]) -&gt; Tuple[TimeStep, LogEnvState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.VecEnvWrapper","title":"<code>src.jaxnasium.VecEnvWrapper</code>","text":"<p>Wrapper to vectorize environments. Simply calls <code>jax.vmap</code> on the <code>reset</code> and <code>step</code> methods of the environment. The number of environmnents is determined by the leading axis of the inputs to the <code>reset</code> and <code>step</code> methods, as if you would call <code>jax.vmap</code> directly.</p> <p>We use a wrapper instead of <code>jax.vmap</code> in each algorithm directly to control where the vectorization happens. This allows other wrappers to act on the vectorized environment, e.g. <code>NormalizeVecObsWrapper</code> and <code>NormalizeVecRewardWrapper</code>.</p>"},{"location":"api/Wrappers/#src.jaxnasium.VecEnvWrapper.step_env","title":"<code>step_env(key: PRNGKeyArray, state: TEnvState, action: PyTree[Real[Array, ...]]) -&gt; Tuple[TimeStep, TEnvState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.VecEnvWrapper.reset_env","title":"<code>reset_env(key: PRNGKeyArray) -&gt; Tuple[TObservation, TEnvState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.VecEnvWrapper.action_space","title":"<code>action_space: Space | PyTree[Space]</code>  <code>property</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.VecEnvWrapper.observation_space","title":"<code>observation_space: Space | PyTree[Space]</code>  <code>property</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.VecEnvWrapper.auto_reset","title":"<code>auto_reset(key: PRNGKeyArray, timestep_step: TimeStep, state_step: TEnvState) -&gt; Tuple[TimeStep, TEnvState]</code>","text":"<p>Auto-resets the environment when the episode is terminated or truncated.</p> <p>Given a step timestep and state, this function will auto-reset the environment and return the new timestep and state when the episode is terminated or truncated. Inserts the original observation in info to bootstrap correctly on truncated episodes.</p> <p>Arguments:</p> <ul> <li><code>key</code>: JAX PRNG key.</li> <li><code>timestep_step</code>: The timestep returned by the <code>step_env</code> method.</li> <li><code>state_step</code>: The state returned by the <code>step_env</code> method.</li> </ul> <p>Returns: A tuple of the new timestep and state with the state and observation reset to a new initial state and observation when the episode is terminated or truncated. The original observation is inserted in info to bootstrap correctly on truncated episodes.</p>"},{"location":"api/Wrappers/#src.jaxnasium.VecEnvWrapper.sample_action","title":"<code>sample_action(key: PRNGKeyArray) -&gt; PyTree[Real[Array, ...]]</code>","text":"<p>Convenience method to sample a random action from the environment's action space. While one could use <code>self.action_space.sample(key)</code>, this method additionally works on composite action spaces.</p>"},{"location":"api/Wrappers/#src.jaxnasium.VecEnvWrapper.sample_observation","title":"<code>sample_observation(key: PRNGKeyArray) -&gt; TObservation</code>","text":"<p>Convenience method to sample a random observation from the environment's observation space. While one could use <code>self.observation_space.sample(key)</code>, this method additionally works on composite observation spaces.</p>"},{"location":"api/Wrappers/#src.jaxnasium.VecEnvWrapper.multi_agent","title":"<code>multi_agent: bool</code>  <code>property</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.VecEnvWrapper.agent_structure","title":"<code>agent_structure: PyTreeDef</code>  <code>property</code>","text":"<p>Returns the structure of the agent space. In single-agent environments this is simply a PyTreeDef(). However, for multi-agent environments this is a PyTreeDef((, x num_agents)).</p>"},{"location":"api/Wrappers/#src.jaxnasium.VecEnvWrapper.__getattr__","title":"<code>__getattr__(name)</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.VecEnvWrapper.reset","title":"<code>reset(key: PRNGKeyArray) -&gt; Tuple[TObservation, Any]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.VecEnvWrapper.step","title":"<code>step(key: PRNGKeyArray, state: TEnvState, action: PyTree[Real[Array, ...]]) -&gt; Tuple[TimeStep, TEnvState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.NormalizeVecObsWrapper","title":"<code>src.jaxnasium.NormalizeVecObsWrapper</code>","text":"<p>Normalize the observations of the environment via running mean and variance. This wrapper acts on vectorized environments and in turn should be wrapped within a <code>VecEnvWrapper</code>.</p> <p>Arguments:</p> <ul> <li><code>_env</code>: Environment to wrap.</li> </ul>"},{"location":"api/Wrappers/#src.jaxnasium.NormalizeVecObsWrapper.step_env","title":"<code>step_env(key: PRNGKeyArray, state: TEnvState, action: PyTree[Real[Array, ...]]) -&gt; Tuple[TimeStep, TEnvState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.NormalizeVecObsWrapper.reset_env","title":"<code>reset_env(key: PRNGKeyArray) -&gt; Tuple[TObservation, TEnvState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.NormalizeVecObsWrapper.action_space","title":"<code>action_space: Space | PyTree[Space]</code>  <code>property</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.NormalizeVecObsWrapper.observation_space","title":"<code>observation_space: Space | PyTree[Space]</code>  <code>property</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.NormalizeVecObsWrapper.auto_reset","title":"<code>auto_reset(key: PRNGKeyArray, timestep_step: TimeStep, state_step: TEnvState) -&gt; Tuple[TimeStep, TEnvState]</code>","text":"<p>Auto-resets the environment when the episode is terminated or truncated.</p> <p>Given a step timestep and state, this function will auto-reset the environment and return the new timestep and state when the episode is terminated or truncated. Inserts the original observation in info to bootstrap correctly on truncated episodes.</p> <p>Arguments:</p> <ul> <li><code>key</code>: JAX PRNG key.</li> <li><code>timestep_step</code>: The timestep returned by the <code>step_env</code> method.</li> <li><code>state_step</code>: The state returned by the <code>step_env</code> method.</li> </ul> <p>Returns: A tuple of the new timestep and state with the state and observation reset to a new initial state and observation when the episode is terminated or truncated. The original observation is inserted in info to bootstrap correctly on truncated episodes.</p>"},{"location":"api/Wrappers/#src.jaxnasium.NormalizeVecObsWrapper.sample_action","title":"<code>sample_action(key: PRNGKeyArray) -&gt; PyTree[Real[Array, ...]]</code>","text":"<p>Convenience method to sample a random action from the environment's action space. While one could use <code>self.action_space.sample(key)</code>, this method additionally works on composite action spaces.</p>"},{"location":"api/Wrappers/#src.jaxnasium.NormalizeVecObsWrapper.sample_observation","title":"<code>sample_observation(key: PRNGKeyArray) -&gt; TObservation</code>","text":"<p>Convenience method to sample a random observation from the environment's observation space. While one could use <code>self.observation_space.sample(key)</code>, this method additionally works on composite observation spaces.</p>"},{"location":"api/Wrappers/#src.jaxnasium.NormalizeVecObsWrapper.multi_agent","title":"<code>multi_agent: bool</code>  <code>property</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.NormalizeVecObsWrapper.agent_structure","title":"<code>agent_structure: PyTreeDef</code>  <code>property</code>","text":"<p>Returns the structure of the agent space. In single-agent environments this is simply a PyTreeDef(). However, for multi-agent environments this is a PyTreeDef((, x num_agents)).</p>"},{"location":"api/Wrappers/#src.jaxnasium.NormalizeVecObsWrapper.__getattr__","title":"<code>__getattr__(name)</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.NormalizeVecObsWrapper.__check_init__","title":"<code>__check_init__()</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.NormalizeVecObsWrapper.update_state_and_get_obs","title":"<code>update_state_and_get_obs(obs, state: NormalizeVecObsState)</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.NormalizeVecObsWrapper.reset","title":"<code>reset(key: PRNGKeyArray) -&gt; Tuple[TObservation, NormalizeVecObsState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.NormalizeVecObsWrapper.step","title":"<code>step(key: PRNGKeyArray, state: NormalizeVecObsState, action: PyTree[int | float | Array]) -&gt; Tuple[TimeStep, NormalizeVecObsState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.NormalizeVecRewardWrapper","title":"<code>src.jaxnasium.NormalizeVecRewardWrapper</code>","text":"<p>Normalize the rewards of the environment via running mean and variance. This wrapper acts on vectorized environments and in turn should be wrapped within a <code>VecEnvWrapper</code>.</p> <p>Arguments:</p> <ul> <li><code>_env</code>: Environment to wrap.</li> <li><code>gamma</code>: Discount factor for the rewards.</li> </ul>"},{"location":"api/Wrappers/#src.jaxnasium.NormalizeVecRewardWrapper.step_env","title":"<code>step_env(key: PRNGKeyArray, state: TEnvState, action: PyTree[Real[Array, ...]]) -&gt; Tuple[TimeStep, TEnvState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.NormalizeVecRewardWrapper.reset_env","title":"<code>reset_env(key: PRNGKeyArray) -&gt; Tuple[TObservation, TEnvState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.NormalizeVecRewardWrapper.action_space","title":"<code>action_space: Space | PyTree[Space]</code>  <code>property</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.NormalizeVecRewardWrapper.observation_space","title":"<code>observation_space: Space | PyTree[Space]</code>  <code>property</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.NormalizeVecRewardWrapper.auto_reset","title":"<code>auto_reset(key: PRNGKeyArray, timestep_step: TimeStep, state_step: TEnvState) -&gt; Tuple[TimeStep, TEnvState]</code>","text":"<p>Auto-resets the environment when the episode is terminated or truncated.</p> <p>Given a step timestep and state, this function will auto-reset the environment and return the new timestep and state when the episode is terminated or truncated. Inserts the original observation in info to bootstrap correctly on truncated episodes.</p> <p>Arguments:</p> <ul> <li><code>key</code>: JAX PRNG key.</li> <li><code>timestep_step</code>: The timestep returned by the <code>step_env</code> method.</li> <li><code>state_step</code>: The state returned by the <code>step_env</code> method.</li> </ul> <p>Returns: A tuple of the new timestep and state with the state and observation reset to a new initial state and observation when the episode is terminated or truncated. The original observation is inserted in info to bootstrap correctly on truncated episodes.</p>"},{"location":"api/Wrappers/#src.jaxnasium.NormalizeVecRewardWrapper.sample_action","title":"<code>sample_action(key: PRNGKeyArray) -&gt; PyTree[Real[Array, ...]]</code>","text":"<p>Convenience method to sample a random action from the environment's action space. While one could use <code>self.action_space.sample(key)</code>, this method additionally works on composite action spaces.</p>"},{"location":"api/Wrappers/#src.jaxnasium.NormalizeVecRewardWrapper.sample_observation","title":"<code>sample_observation(key: PRNGKeyArray) -&gt; TObservation</code>","text":"<p>Convenience method to sample a random observation from the environment's observation space. While one could use <code>self.observation_space.sample(key)</code>, this method additionally works on composite observation spaces.</p>"},{"location":"api/Wrappers/#src.jaxnasium.NormalizeVecRewardWrapper.multi_agent","title":"<code>multi_agent: bool</code>  <code>property</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.NormalizeVecRewardWrapper.agent_structure","title":"<code>agent_structure: PyTreeDef</code>  <code>property</code>","text":"<p>Returns the structure of the agent space. In single-agent environments this is simply a PyTreeDef(). However, for multi-agent environments this is a PyTreeDef((, x num_agents)).</p>"},{"location":"api/Wrappers/#src.jaxnasium.NormalizeVecRewardWrapper.__getattr__","title":"<code>__getattr__(name)</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.NormalizeVecRewardWrapper.gamma","title":"<code>gamma: float = 0.99</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.NormalizeVecRewardWrapper.__check_init__","title":"<code>__check_init__()</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.NormalizeVecRewardWrapper.reset","title":"<code>reset(key: PRNGKeyArray) -&gt; Tuple[TObservation, NormalizeVecRewState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.NormalizeVecRewardWrapper.step","title":"<code>step(key: PRNGKeyArray, state: NormalizeVecRewState, action: PyTree[int | float | Array]) -&gt; Tuple[TimeStep, NormalizeVecRewState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.FlattenObservationWrapper","title":"<code>src.jaxnasium.FlattenObservationWrapper</code>","text":"<p>Flatten the observations of the environment.</p> <p>Flattens each observation in the environment to a single vector. When the observation is a PyTree of arrays, it flattens each array and returns the same PyTree structure with the flattened arrays.</p> <p>Arguments:</p> <ul> <li><code>_env</code>: Environment to wrap.</li> </ul>"},{"location":"api/Wrappers/#src.jaxnasium.FlattenObservationWrapper.step_env","title":"<code>step_env(key: PRNGKeyArray, state: TEnvState, action: PyTree[Real[Array, ...]]) -&gt; Tuple[TimeStep, TEnvState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.FlattenObservationWrapper.reset_env","title":"<code>reset_env(key: PRNGKeyArray) -&gt; Tuple[TObservation, TEnvState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.FlattenObservationWrapper.action_space","title":"<code>action_space: Space | PyTree[Space]</code>  <code>property</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.FlattenObservationWrapper.auto_reset","title":"<code>auto_reset(key: PRNGKeyArray, timestep_step: TimeStep, state_step: TEnvState) -&gt; Tuple[TimeStep, TEnvState]</code>","text":"<p>Auto-resets the environment when the episode is terminated or truncated.</p> <p>Given a step timestep and state, this function will auto-reset the environment and return the new timestep and state when the episode is terminated or truncated. Inserts the original observation in info to bootstrap correctly on truncated episodes.</p> <p>Arguments:</p> <ul> <li><code>key</code>: JAX PRNG key.</li> <li><code>timestep_step</code>: The timestep returned by the <code>step_env</code> method.</li> <li><code>state_step</code>: The state returned by the <code>step_env</code> method.</li> </ul> <p>Returns: A tuple of the new timestep and state with the state and observation reset to a new initial state and observation when the episode is terminated or truncated. The original observation is inserted in info to bootstrap correctly on truncated episodes.</p>"},{"location":"api/Wrappers/#src.jaxnasium.FlattenObservationWrapper.sample_action","title":"<code>sample_action(key: PRNGKeyArray) -&gt; PyTree[Real[Array, ...]]</code>","text":"<p>Convenience method to sample a random action from the environment's action space. While one could use <code>self.action_space.sample(key)</code>, this method additionally works on composite action spaces.</p>"},{"location":"api/Wrappers/#src.jaxnasium.FlattenObservationWrapper.sample_observation","title":"<code>sample_observation(key: PRNGKeyArray) -&gt; TObservation</code>","text":"<p>Convenience method to sample a random observation from the environment's observation space. While one could use <code>self.observation_space.sample(key)</code>, this method additionally works on composite observation spaces.</p>"},{"location":"api/Wrappers/#src.jaxnasium.FlattenObservationWrapper.multi_agent","title":"<code>multi_agent: bool</code>  <code>property</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.FlattenObservationWrapper.agent_structure","title":"<code>agent_structure: PyTreeDef</code>  <code>property</code>","text":"<p>Returns the structure of the agent space. In single-agent environments this is simply a PyTreeDef(). However, for multi-agent environments this is a PyTreeDef((, x num_agents)).</p>"},{"location":"api/Wrappers/#src.jaxnasium.FlattenObservationWrapper.__getattr__","title":"<code>__getattr__(name)</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.FlattenObservationWrapper.reset","title":"<code>reset(key: PRNGKeyArray) -&gt; Tuple[TObservation, TEnvState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.FlattenObservationWrapper.step","title":"<code>step(key: PRNGKeyArray, state: TEnvState, action: PyTree[int | float | Array]) -&gt; Tuple[TimeStep, TEnvState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.FlattenObservationWrapper.observation_space","title":"<code>observation_space: Space</code>  <code>property</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.TransformRewardWrapper","title":"<code>src.jaxnasium.TransformRewardWrapper</code>","text":"<p>Transform the rewards of the environment using a given function.</p> <p>Arguments:</p> <ul> <li><code>_env</code>: Environment to wrap.</li> <li><code>transform_fn</code>: Function to transform the rewards.</li> </ul>"},{"location":"api/Wrappers/#src.jaxnasium.TransformRewardWrapper.reset","title":"<code>reset(key: PRNGKeyArray) -&gt; Tuple[TObservation, Any]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.TransformRewardWrapper.step_env","title":"<code>step_env(key: PRNGKeyArray, state: TEnvState, action: PyTree[Real[Array, ...]]) -&gt; Tuple[TimeStep, TEnvState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.TransformRewardWrapper.reset_env","title":"<code>reset_env(key: PRNGKeyArray) -&gt; Tuple[TObservation, TEnvState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.TransformRewardWrapper.action_space","title":"<code>action_space: Space | PyTree[Space]</code>  <code>property</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.TransformRewardWrapper.observation_space","title":"<code>observation_space: Space | PyTree[Space]</code>  <code>property</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.TransformRewardWrapper.auto_reset","title":"<code>auto_reset(key: PRNGKeyArray, timestep_step: TimeStep, state_step: TEnvState) -&gt; Tuple[TimeStep, TEnvState]</code>","text":"<p>Auto-resets the environment when the episode is terminated or truncated.</p> <p>Given a step timestep and state, this function will auto-reset the environment and return the new timestep and state when the episode is terminated or truncated. Inserts the original observation in info to bootstrap correctly on truncated episodes.</p> <p>Arguments:</p> <ul> <li><code>key</code>: JAX PRNG key.</li> <li><code>timestep_step</code>: The timestep returned by the <code>step_env</code> method.</li> <li><code>state_step</code>: The state returned by the <code>step_env</code> method.</li> </ul> <p>Returns: A tuple of the new timestep and state with the state and observation reset to a new initial state and observation when the episode is terminated or truncated. The original observation is inserted in info to bootstrap correctly on truncated episodes.</p>"},{"location":"api/Wrappers/#src.jaxnasium.TransformRewardWrapper.sample_action","title":"<code>sample_action(key: PRNGKeyArray) -&gt; PyTree[Real[Array, ...]]</code>","text":"<p>Convenience method to sample a random action from the environment's action space. While one could use <code>self.action_space.sample(key)</code>, this method additionally works on composite action spaces.</p>"},{"location":"api/Wrappers/#src.jaxnasium.TransformRewardWrapper.sample_observation","title":"<code>sample_observation(key: PRNGKeyArray) -&gt; TObservation</code>","text":"<p>Convenience method to sample a random observation from the environment's observation space. While one could use <code>self.observation_space.sample(key)</code>, this method additionally works on composite observation spaces.</p>"},{"location":"api/Wrappers/#src.jaxnasium.TransformRewardWrapper.multi_agent","title":"<code>multi_agent: bool</code>  <code>property</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.TransformRewardWrapper.agent_structure","title":"<code>agent_structure: PyTreeDef</code>  <code>property</code>","text":"<p>Returns the structure of the agent space. In single-agent environments this is simply a PyTreeDef(). However, for multi-agent environments this is a PyTreeDef((, x num_agents)).</p>"},{"location":"api/Wrappers/#src.jaxnasium.TransformRewardWrapper.__getattr__","title":"<code>__getattr__(name)</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.TransformRewardWrapper.transform_fn","title":"<code>transform_fn: Callable</code>  <code>instance-attribute</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.TransformRewardWrapper.step","title":"<code>step(key: PRNGKeyArray, state: TEnvState, action: PyTree[int | float | Array]) -&gt; Tuple[TimeStep, TEnvState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.ScaleRewardWrapper","title":"<code>src.jaxnasium.ScaleRewardWrapper</code>","text":"<p>Scale the rewards of the environment by a given factor.</p> <p>Arguments:</p> <ul> <li><code>_env</code>: Environment to wrap.</li> <li><code>scale</code>: Factor to scale the rewards by.</li> </ul>"},{"location":"api/Wrappers/#src.jaxnasium.ScaleRewardWrapper.step","title":"<code>step(key: PRNGKeyArray, state: TEnvState, action: PyTree[int | float | Array]) -&gt; Tuple[TimeStep, TEnvState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.ScaleRewardWrapper.reset","title":"<code>reset(key: PRNGKeyArray) -&gt; Tuple[TObservation, Any]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.ScaleRewardWrapper.step_env","title":"<code>step_env(key: PRNGKeyArray, state: TEnvState, action: PyTree[Real[Array, ...]]) -&gt; Tuple[TimeStep, TEnvState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.ScaleRewardWrapper.reset_env","title":"<code>reset_env(key: PRNGKeyArray) -&gt; Tuple[TObservation, TEnvState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.ScaleRewardWrapper.action_space","title":"<code>action_space: Space | PyTree[Space]</code>  <code>property</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.ScaleRewardWrapper.observation_space","title":"<code>observation_space: Space | PyTree[Space]</code>  <code>property</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.ScaleRewardWrapper.auto_reset","title":"<code>auto_reset(key: PRNGKeyArray, timestep_step: TimeStep, state_step: TEnvState) -&gt; Tuple[TimeStep, TEnvState]</code>","text":"<p>Auto-resets the environment when the episode is terminated or truncated.</p> <p>Given a step timestep and state, this function will auto-reset the environment and return the new timestep and state when the episode is terminated or truncated. Inserts the original observation in info to bootstrap correctly on truncated episodes.</p> <p>Arguments:</p> <ul> <li><code>key</code>: JAX PRNG key.</li> <li><code>timestep_step</code>: The timestep returned by the <code>step_env</code> method.</li> <li><code>state_step</code>: The state returned by the <code>step_env</code> method.</li> </ul> <p>Returns: A tuple of the new timestep and state with the state and observation reset to a new initial state and observation when the episode is terminated or truncated. The original observation is inserted in info to bootstrap correctly on truncated episodes.</p>"},{"location":"api/Wrappers/#src.jaxnasium.ScaleRewardWrapper.sample_action","title":"<code>sample_action(key: PRNGKeyArray) -&gt; PyTree[Real[Array, ...]]</code>","text":"<p>Convenience method to sample a random action from the environment's action space. While one could use <code>self.action_space.sample(key)</code>, this method additionally works on composite action spaces.</p>"},{"location":"api/Wrappers/#src.jaxnasium.ScaleRewardWrapper.sample_observation","title":"<code>sample_observation(key: PRNGKeyArray) -&gt; TObservation</code>","text":"<p>Convenience method to sample a random observation from the environment's observation space. While one could use <code>self.observation_space.sample(key)</code>, this method additionally works on composite observation spaces.</p>"},{"location":"api/Wrappers/#src.jaxnasium.ScaleRewardWrapper.multi_agent","title":"<code>multi_agent: bool</code>  <code>property</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.ScaleRewardWrapper.agent_structure","title":"<code>agent_structure: PyTreeDef</code>  <code>property</code>","text":"<p>Returns the structure of the agent space. In single-agent environments this is simply a PyTreeDef(). However, for multi-agent environments this is a PyTreeDef((, x num_agents)).</p>"},{"location":"api/Wrappers/#src.jaxnasium.ScaleRewardWrapper.__getattr__","title":"<code>__getattr__(name)</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.ScaleRewardWrapper.scale","title":"<code>scale: float = scale</code>  <code>instance-attribute</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.ScaleRewardWrapper.__init__","title":"<code>__init__(env: Environment, scale: float = 1.0)</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.ScaleRewardWrapper.transform_fn","title":"<code>transform_fn = lambda r: r * scale</code>  <code>instance-attribute</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.DiscreteActionWrapper","title":"<code>src.jaxnasium.DiscreteActionWrapper</code>","text":"<p>Wrapper to convert continuous actions to discrete actions.</p> <p>Arguments:</p> <ul> <li><code>_env</code>: Environment to wrap.</li> <li><code>num_actions</code>: Number of discrete actions to convert to.</li> </ul>"},{"location":"api/Wrappers/#src.jaxnasium.DiscreteActionWrapper.reset","title":"<code>reset(key: PRNGKeyArray) -&gt; Tuple[TObservation, Any]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.DiscreteActionWrapper.step_env","title":"<code>step_env(key: PRNGKeyArray, state: TEnvState, action: PyTree[Real[Array, ...]]) -&gt; Tuple[TimeStep, TEnvState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.DiscreteActionWrapper.reset_env","title":"<code>reset_env(key: PRNGKeyArray) -&gt; Tuple[TObservation, TEnvState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.DiscreteActionWrapper.observation_space","title":"<code>observation_space: Space | PyTree[Space]</code>  <code>property</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.DiscreteActionWrapper.auto_reset","title":"<code>auto_reset(key: PRNGKeyArray, timestep_step: TimeStep, state_step: TEnvState) -&gt; Tuple[TimeStep, TEnvState]</code>","text":"<p>Auto-resets the environment when the episode is terminated or truncated.</p> <p>Given a step timestep and state, this function will auto-reset the environment and return the new timestep and state when the episode is terminated or truncated. Inserts the original observation in info to bootstrap correctly on truncated episodes.</p> <p>Arguments:</p> <ul> <li><code>key</code>: JAX PRNG key.</li> <li><code>timestep_step</code>: The timestep returned by the <code>step_env</code> method.</li> <li><code>state_step</code>: The state returned by the <code>step_env</code> method.</li> </ul> <p>Returns: A tuple of the new timestep and state with the state and observation reset to a new initial state and observation when the episode is terminated or truncated. The original observation is inserted in info to bootstrap correctly on truncated episodes.</p>"},{"location":"api/Wrappers/#src.jaxnasium.DiscreteActionWrapper.sample_action","title":"<code>sample_action(key: PRNGKeyArray) -&gt; PyTree[Real[Array, ...]]</code>","text":"<p>Convenience method to sample a random action from the environment's action space. While one could use <code>self.action_space.sample(key)</code>, this method additionally works on composite action spaces.</p>"},{"location":"api/Wrappers/#src.jaxnasium.DiscreteActionWrapper.sample_observation","title":"<code>sample_observation(key: PRNGKeyArray) -&gt; TObservation</code>","text":"<p>Convenience method to sample a random observation from the environment's observation space. While one could use <code>self.observation_space.sample(key)</code>, this method additionally works on composite observation spaces.</p>"},{"location":"api/Wrappers/#src.jaxnasium.DiscreteActionWrapper.multi_agent","title":"<code>multi_agent: bool</code>  <code>property</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.DiscreteActionWrapper.agent_structure","title":"<code>agent_structure: PyTreeDef</code>  <code>property</code>","text":"<p>Returns the structure of the agent space. In single-agent environments this is simply a PyTreeDef(). However, for multi-agent environments this is a PyTreeDef((, x num_agents)).</p>"},{"location":"api/Wrappers/#src.jaxnasium.DiscreteActionWrapper.__getattr__","title":"<code>__getattr__(name)</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.DiscreteActionWrapper.num_actions","title":"<code>num_actions: int</code>  <code>instance-attribute</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.DiscreteActionWrapper.step","title":"<code>step(key: PRNGKeyArray, state: TEnvState, action: int | Int[Array, ' num_actions']) -&gt; Tuple[TimeStep, TEnvState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.DiscreteActionWrapper.action_space","title":"<code>action_space: Discrete | MultiDiscrete</code>  <code>property</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.DiscreteActionWrapper.original_action_space","title":"<code>original_action_space: Space</code>  <code>property</code>","text":"<p>Return the original action space of the environment. This is useful for algorithms that need to know the original action space.</p>"},{"location":"api/Wrappers/#src.jaxnasium.FlattenActionSpaceWrapper","title":"<code>src.jaxnasium.FlattenActionSpaceWrapper</code>","text":"<p>Wrapper to convert (PyTrees of) (multi-)discrete action spaces to a single discrete action space. This grows the action space (significantly for large action spaces), but allows to use algorithms that only support discrete action spaces.</p> <p>First flattens each MultiDiscrete action space to a single discrete action space, then combines possibly remaining discrete action spaces to a single discrete action space.</p> <p>Arguments:</p> <ul> <li><code>_env</code>: Environment to wrap.</li> </ul>"},{"location":"api/Wrappers/#src.jaxnasium.FlattenActionSpaceWrapper.reset","title":"<code>reset(key: PRNGKeyArray) -&gt; Tuple[TObservation, Any]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.FlattenActionSpaceWrapper.step_env","title":"<code>step_env(key: PRNGKeyArray, state: TEnvState, action: PyTree[Real[Array, ...]]) -&gt; Tuple[TimeStep, TEnvState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.FlattenActionSpaceWrapper.reset_env","title":"<code>reset_env(key: PRNGKeyArray) -&gt; Tuple[TObservation, TEnvState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.FlattenActionSpaceWrapper.observation_space","title":"<code>observation_space: Space | PyTree[Space]</code>  <code>property</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.FlattenActionSpaceWrapper.auto_reset","title":"<code>auto_reset(key: PRNGKeyArray, timestep_step: TimeStep, state_step: TEnvState) -&gt; Tuple[TimeStep, TEnvState]</code>","text":"<p>Auto-resets the environment when the episode is terminated or truncated.</p> <p>Given a step timestep and state, this function will auto-reset the environment and return the new timestep and state when the episode is terminated or truncated. Inserts the original observation in info to bootstrap correctly on truncated episodes.</p> <p>Arguments:</p> <ul> <li><code>key</code>: JAX PRNG key.</li> <li><code>timestep_step</code>: The timestep returned by the <code>step_env</code> method.</li> <li><code>state_step</code>: The state returned by the <code>step_env</code> method.</li> </ul> <p>Returns: A tuple of the new timestep and state with the state and observation reset to a new initial state and observation when the episode is terminated or truncated. The original observation is inserted in info to bootstrap correctly on truncated episodes.</p>"},{"location":"api/Wrappers/#src.jaxnasium.FlattenActionSpaceWrapper.sample_action","title":"<code>sample_action(key: PRNGKeyArray) -&gt; PyTree[Real[Array, ...]]</code>","text":"<p>Convenience method to sample a random action from the environment's action space. While one could use <code>self.action_space.sample(key)</code>, this method additionally works on composite action spaces.</p>"},{"location":"api/Wrappers/#src.jaxnasium.FlattenActionSpaceWrapper.sample_observation","title":"<code>sample_observation(key: PRNGKeyArray) -&gt; TObservation</code>","text":"<p>Convenience method to sample a random observation from the environment's observation space. While one could use <code>self.observation_space.sample(key)</code>, this method additionally works on composite observation spaces.</p>"},{"location":"api/Wrappers/#src.jaxnasium.FlattenActionSpaceWrapper.multi_agent","title":"<code>multi_agent: bool</code>  <code>property</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.FlattenActionSpaceWrapper.agent_structure","title":"<code>agent_structure: PyTreeDef</code>  <code>property</code>","text":"<p>Returns the structure of the agent space. In single-agent environments this is simply a PyTreeDef(). However, for multi-agent environments this is a PyTreeDef((, x num_agents)).</p>"},{"location":"api/Wrappers/#src.jaxnasium.FlattenActionSpaceWrapper.__getattr__","title":"<code>__getattr__(name)</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.FlattenActionSpaceWrapper.step","title":"<code>step(key: PRNGKeyArray, state: TEnvState, action: int) -&gt; Tuple[TimeStep, TEnvState]</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.FlattenActionSpaceWrapper.action_space","title":"<code>action_space: Discrete</code>  <code>property</code>","text":""},{"location":"api/Wrappers/#src.jaxnasium.FlattenActionSpaceWrapper.original_action_space","title":"<code>original_action_space: Space</code>  <code>property</code>","text":"<p>Return the original action space of the environment.</p>"},{"location":"api/Wrappers/#utility-functions","title":"Utility functions","text":""},{"location":"api/Wrappers/#src.jaxnasium.is_wrapped","title":"<code>src.jaxnasium.is_wrapped(wrapped_env: Environment, wrapper_class: type | str) -&gt; bool</code>","text":"<p>Check if the environment is wrapped with a specific wrapper class.</p>"},{"location":"api/Wrappers/#src.jaxnasium.remove_wrapper","title":"<code>src.jaxnasium.remove_wrapper(wrapped_env: Environment, wrapper_class: type) -&gt; Environment</code>","text":"<p>Remove a specific wrapper class from the environment.</p>"},{"location":"api/_Available-Environments-List/","title":"Available Environments List","text":"<ul> <li><code>Acrobot-v1</code></li> <li><code>CartPole-v1</code></li> <li><code>MountainCarContinuous-v0</code></li> <li><code>MountainCar-v0</code></li> <li><code>Pendulum-v1</code></li> <li><code>gymnax:CartPole-v1</code></li> <li><code>gymnax:Acrobot-v1</code></li> <li><code>gymnax:Pendulum-v1</code></li> <li><code>gymnax:MountainCar-v0</code></li> <li><code>gymnax:MountainCarContinuous-v0</code></li> <li><code>gymnax:Asterix-MinAtar</code></li> <li><code>gymnax:Breakout-MinAtar</code></li> <li><code>gymnax:Freeway-MinAtar</code></li> <li><code>gymnax:SpaceInvaders-MinAtar</code></li> <li><code>gymnax:DeepSea-bsuite</code></li> <li><code>gymnax:Catch-bsuite</code></li> <li><code>gymnax:MemoryChain-bsuite</code></li> <li><code>gymnax:UmbrellaChain-bsuite</code></li> <li><code>gymnax:DiscountingChain-bsuite</code></li> <li><code>gymnax:MNISTBandit-bsuite</code></li> <li><code>gymnax:SimpleBandit-bsuite</code></li> <li><code>gymnax:FourRooms-misc</code></li> <li><code>gymnax:MetaMaze-misc</code></li> <li><code>gymnax:PointRobot-misc</code></li> <li><code>gymnax:BernoulliBandit-misc</code></li> <li><code>gymnax:GaussianBandit-misc</code></li> <li><code>gymnax:Reacher-misc</code></li> <li><code>gymnax:Swimmer-misc</code></li> <li><code>gymnax:Pong-misc</code></li> <li><code>jumanji:Game2048-v1</code></li> <li><code>jumanji:GraphColoring-v1</code></li> <li><code>jumanji:Minesweeper-v0</code></li> <li><code>jumanji:RubiksCube-v0</code></li> <li><code>jumanji:RubiksCube-partly-scrambled-v0</code></li> <li><code>jumanji:SlidingTilePuzzle-v0</code></li> <li><code>jumanji:Sudoku-v0</code></li> <li><code>jumanji:Sudoku-very-easy-v0</code></li> <li><code>jumanji:BinPack-v2</code></li> <li><code>jumanji:FlatPack-v0</code></li> <li><code>jumanji:JobShop-v0</code></li> <li><code>jumanji:Knapsack-v1</code></li> <li><code>jumanji:Tetris-v0</code></li> <li><code>jumanji:Cleaner-v0</code></li> <li><code>jumanji:Connector-v2</code></li> <li><code>jumanji:CVRP-v1</code></li> <li><code>jumanji:MultiCVRP-v0</code></li> <li><code>jumanji:Maze-v0</code></li> <li><code>jumanji:RobotWarehouse-v0</code></li> <li><code>jumanji:Snake-v1</code></li> <li><code>jumanji:TSP-v1</code></li> <li><code>jumanji:MMST-v0</code></li> <li><code>jumanji:PacMan-v1</code></li> <li><code>jumanji:Sokoban-v0</code></li> <li><code>jumanji:LevelBasedForaging-v0</code></li> <li><code>jumanji:SearchAndRescue-v0</code></li> <li><code>brax:ant</code></li> <li><code>brax:halfcheetah</code></li> <li><code>brax:hopper</code></li> <li><code>brax:humanoid</code></li> <li><code>brax:humanoidstandup</code></li> <li><code>brax:inverted_pendulum</code></li> <li><code>brax:inverted_double_pendulum</code></li> <li><code>brax:pusher</code></li> <li><code>brax:reacher</code></li> <li><code>brax:walker2d</code></li> <li><code>pgx:2048</code></li> <li><code>pgx:animal_shogi</code></li> <li><code>pgx:backgammon</code></li> <li><code>pgx:chess</code></li> <li><code>pgx:connect_four</code></li> <li><code>pgx:gardner_chess</code></li> <li><code>pgx:go_9x9</code></li> <li><code>pgx:go_19x19</code></li> <li><code>pgx:hex</code></li> <li><code>pgx:kuhn_poker</code></li> <li><code>pgx:leduc_holdem</code></li> <li><code>pgx:minatar-asterix</code></li> <li><code>pgx:minatar-breakout</code></li> <li><code>pgx:minatar-freeway</code></li> <li><code>pgx:minatar-seaquest</code></li> <li><code>pgx:minatar-space_invaders</code></li> <li><code>pgx:othello</code></li> <li><code>pgx:shogi</code></li> <li><code>pgx:sparrow_mahjong</code></li> <li><code>pgx:tic_tac_toe</code></li> <li><code>jaxmarl:MPE_simple_v3</code></li> <li><code>jaxmarl:MPE_simple_tag_v3</code></li> <li><code>jaxmarl:MPE_simple_world_comm_v3</code></li> <li><code>jaxmarl:MPE_simple_spread_v3</code></li> <li><code>jaxmarl:MPE_simple_crypto_v3</code></li> <li><code>jaxmarl:MPE_simple_speaker_listener_v4</code></li> <li><code>jaxmarl:MPE_simple_push_v3</code></li> <li><code>jaxmarl:MPE_simple_adversary_v3</code></li> <li><code>jaxmarl:MPE_simple_reference_v3</code></li> <li><code>jaxmarl:MPE_simple_facmac_v1</code></li> <li><code>jaxmarl:MPE_simple_facmac_3a_v1</code></li> <li><code>jaxmarl:MPE_simple_facmac_6a_v1</code></li> <li><code>jaxmarl:MPE_simple_facmac_9a_v1</code></li> <li><code>jaxmarl:switch_riddle</code></li> <li><code>jaxmarl:SMAX</code></li> <li><code>jaxmarl:HeuristicEnemySMAX</code></li> <li><code>jaxmarl:ant_4x2</code></li> <li><code>jaxmarl:halfcheetah_6x1</code></li> <li><code>jaxmarl:hopper_3x1</code></li> <li><code>jaxmarl:humanoid_9|8</code></li> <li><code>jaxmarl:walker2d_2x3</code></li> <li><code>jaxmarl:storm</code></li> <li><code>jaxmarl:storm_2p</code></li> <li><code>jaxmarl:storm_np</code></li> <li><code>jaxmarl:hanabi</code></li> <li><code>jaxmarl:overcooked</code></li> <li><code>jaxmarl:overcooked_v2</code></li> <li><code>jaxmarl:coin_game</code></li> <li><code>jaxmarl:jaxnav</code></li> <li><code>xminigrid:XLand-MiniGrid-R1-9x9</code></li> <li><code>xminigrid:XLand-MiniGrid-R1-11x11</code></li> <li><code>xminigrid:XLand-MiniGrid-R1-13x13</code></li> <li><code>xminigrid:XLand-MiniGrid-R1-15x15</code></li> <li><code>xminigrid:XLand-MiniGrid-R1-17x17</code></li> <li><code>xminigrid:XLand-MiniGrid-R2-9x9</code></li> <li><code>xminigrid:XLand-MiniGrid-R2-11x11</code></li> <li><code>xminigrid:XLand-MiniGrid-R2-13x13</code></li> <li><code>xminigrid:XLand-MiniGrid-R2-15x15</code></li> <li><code>xminigrid:XLand-MiniGrid-R2-17x17</code></li> <li><code>xminigrid:XLand-MiniGrid-R4-9x9</code></li> <li><code>xminigrid:XLand-MiniGrid-R4-11x11</code></li> <li><code>xminigrid:XLand-MiniGrid-R4-13x13</code></li> <li><code>xminigrid:XLand-MiniGrid-R4-15x15</code></li> <li><code>xminigrid:XLand-MiniGrid-R4-17x17</code></li> <li><code>xminigrid:XLand-MiniGrid-R6-13x13</code></li> <li><code>xminigrid:XLand-MiniGrid-R6-17x17</code></li> <li><code>xminigrid:XLand-MiniGrid-R6-19x19</code></li> <li><code>xminigrid:XLand-MiniGrid-R9-16x16</code></li> <li><code>xminigrid:XLand-MiniGrid-R9-19x19</code></li> <li><code>xminigrid:XLand-MiniGrid-R9-25x25</code></li> <li><code>xminigrid:MiniGrid-BlockedUnlockPickUp</code></li> <li><code>xminigrid:MiniGrid-DoorKey-5x5</code></li> <li><code>xminigrid:MiniGrid-DoorKey-6x6</code></li> <li><code>xminigrid:MiniGrid-DoorKey-8x8</code></li> <li><code>xminigrid:MiniGrid-DoorKey-16x16</code></li> <li><code>xminigrid:MiniGrid-Empty-5x5</code></li> <li><code>xminigrid:MiniGrid-Empty-6x6</code></li> <li><code>xminigrid:MiniGrid-Empty-8x8</code></li> <li><code>xminigrid:MiniGrid-Empty-16x16</code></li> <li><code>xminigrid:MiniGrid-EmptyRandom-5x5</code></li> <li><code>xminigrid:MiniGrid-EmptyRandom-6x6</code></li> <li><code>xminigrid:MiniGrid-EmptyRandom-8x8</code></li> <li><code>xminigrid:MiniGrid-EmptyRandom-16x16</code></li> <li><code>xminigrid:MiniGrid-FourRooms</code></li> <li><code>xminigrid:MiniGrid-LockedRoom</code></li> <li><code>xminigrid:MiniGrid-MemoryS8</code></li> <li><code>xminigrid:MiniGrid-MemoryS16</code></li> <li><code>xminigrid:MiniGrid-MemoryS32</code></li> <li><code>xminigrid:MiniGrid-MemoryS64</code></li> <li><code>xminigrid:MiniGrid-MemoryS128</code></li> <li><code>xminigrid:MiniGrid-Playground</code></li> <li><code>xminigrid:MiniGrid-Unlock</code></li> <li><code>xminigrid:MiniGrid-UnlockPickUp</code></li> <li><code>navix:Navix-Empty-5x5-v0</code></li> <li><code>navix:Navix-Empty-6x6-v0</code></li> <li><code>navix:Navix-Empty-8x8-v0</code></li> <li><code>navix:Navix-Empty-16x16-v0</code></li> <li><code>navix:Navix-Empty-Random-5x5-v0</code></li> <li><code>navix:Navix-Empty-Random-6x6-v0</code></li> <li><code>navix:Navix-Empty-Random-8x8-v0</code></li> <li><code>navix:Navix-Empty-Random-16x16-v0</code></li> <li><code>navix:Navix-DoorKey-5x5-v0</code></li> <li><code>navix:Navix-DoorKey-6x6-v0</code></li> <li><code>navix:Navix-DoorKey-8x8-v0</code></li> <li><code>navix:Navix-DoorKey-16x16-v0</code></li> <li><code>navix:Navix-DoorKey-Random-5x5-v0</code></li> <li><code>navix:Navix-DoorKey-Random-6x6-v0</code></li> <li><code>navix:Navix-DoorKey-Random-8x8-v0</code></li> <li><code>navix:Navix-DoorKey-Random-16x16-v0</code></li> <li><code>navix:Navix-FourRooms-v0</code></li> <li><code>navix:Navix-KeyCorridorS3R1-v0</code></li> <li><code>navix:Navix-KeyCorridorS3R2-v0</code></li> <li><code>navix:Navix-KeyCorridorS3R3-v0</code></li> <li><code>navix:Navix-KeyCorridorS4R3-v0</code></li> <li><code>navix:Navix-KeyCorridorS5R3-v0</code></li> <li><code>navix:Navix-KeyCorridorS6R3-v0</code></li> <li><code>navix:Navix-LavaGapS5-v0</code></li> <li><code>navix:Navix-LavaGapS6-v0</code></li> <li><code>navix:Navix-LavaGapS7-v0</code></li> <li><code>navix:Navix-SimpleCrossingS9N1-v0</code></li> <li><code>navix:Navix-SimpleCrossingS9N2-v0</code></li> <li><code>navix:Navix-SimpleCrossingS9N3-v0</code></li> <li><code>navix:Navix-SimpleCrossingS11N5-v0</code></li> <li><code>navix:Navix-Dynamic-Obstacles-5x5-v0</code></li> <li><code>navix:Navix-Dynamic-Obstacles-5x5-Random-v0</code></li> <li><code>navix:Navix-Dynamic-Obstacles-6x6-v0</code></li> <li><code>navix:Navix-Dynamic-Obstacles-6x6-Random-v0</code></li> <li><code>navix:Navix-Dynamic-Obstacles-8x8-v0</code></li> <li><code>navix:Navix-Dynamic-Obstacles-16x16-v0</code></li> <li><code>navix:Navix-DistShift1-v0</code></li> <li><code>navix:Navix-DistShift2-v0</code></li> <li><code>navix:Navix-GoToDoor-5x5-v0</code></li> <li><code>navix:Navix-GoToDoor-6x6-v0</code></li> <li><code>navix:Navix-GoToDoor-8x8-v0</code></li> <li><code>craftax:Craftax-Classic-Symbolic-v1</code></li> <li><code>craftax:Craftax-Classic-Pixels-v1</code></li> <li><code>craftax:Craftax-Symbolic-v1</code></li> <li><code>craftax:Craftax-Pixels-v1</code></li> </ul>"},{"location":"tree/Tree/","title":"Tree","text":"<p>The <code>jaxnasium.tree</code> package is provided convenience pytree functions used in the various RL algorithms which  aren't found in used higher-level libraries (equinox / jax / optax).</p>"},{"location":"tree/Tree/#src.jaxnasium.tree._tree","title":"<code>src.jaxnasium.tree._tree</code>","text":""},{"location":"tree/Tree/#src.jaxnasium.tree._tree.tree_mean","title":"<code>tree_mean(tree)</code>","text":"<p>Computes the global mean of the leaves of a pytree.</p>"},{"location":"tree/Tree/#src.jaxnasium.tree._tree.tree_get_first","title":"<code>tree_get_first(tree: PyTree, key: str) -&gt; Any</code>","text":"<p>Get the first value from a pytree with the given key. Like <code>optax.tree.get()</code> but returns the first value found in case of multiple matches instead of raising an error.</p> <p>Arguments:</p> <ul> <li><code>tree</code>: A pytree.</li> <li><code>key</code>: A string key.</li> </ul> <p>Returns:     The first value from the pytree with the given key.</p> <p>Raises:     KeyError: If the key is not found in the pytree.</p>"},{"location":"tree/Tree/#src.jaxnasium.tree._tree.tree_map_one_level","title":"<code>tree_map_one_level(fn: Callable, tree, *rest)</code>","text":"<p>Simple <code>jax.tree.map</code> operation over the first level of a pytree.</p> <p>Arguments:</p> <ul> <li><code>fn</code>: A function to map over the pytree.</li> <li><code>tree</code>: A pytree.</li> <li><code>*rest</code>: Additional pytrees to map over.</li> </ul>"},{"location":"tree/Tree/#src.jaxnasium.tree._tree.tree_map_distribution","title":"<code>tree_map_distribution(fn: Callable, tree, *rest)</code>","text":"<p>Map a function with <code>distrax.Distribution</code> instances marked as leaves. Additionally, if one of the inputs is a <code>DistraxContainer</code>, the function is applied to the <code>distribution</code> attribute of the <code>DistraxContainer</code>.</p> <p>Arguments:</p> <ul> <li><code>fn</code>: A function to map over the pytree.</li> <li><code>tree</code>: A pytree.</li> <li><code>*rest</code>: Additional pytrees to map over.</li> </ul>"},{"location":"tree/Tree/#src.jaxnasium.tree._tree.tree_stack","title":"<code>tree_stack(pytrees: PyTree, *, axis=0) -&gt; PyTree</code>","text":"<p>Stack corresponding leaves of pytrees along the specified axis.</p> <p>Interprets the root node's immediate children as a batch of N pytrees that all share the same structure. For each leaf, stacks the N leaves along <code>axis</code> using jnp.stack. This does not traverse deeper than one level when determining what to stack.</p> <p>Arguments:</p> <ul> <li><code>pytrees</code>: A pytree whose root has N immediate children. Each child must have     the same pytree structure. Corresponding leaves must be array-like and     have identical shapes and dtypes (compatible with jnp.stack).</li> <li><code>axis</code>: Axis along which to insert the new dimension of size N in each stacked leaf (default=0).</li> </ul> <p>Returns:     A pytree with the same structure as a single direct-child element of <code>pytrees</code>, where each     leaf is the stack of the corresponding leaves across all elements, with a new     dimension of size N inserted at <code>axis</code>.</p> <p>Example: <pre><code>    &gt;&gt;&gt; trees = (\n    ...     [jnp.array([1, 2]), jnp.array(4)],\n    ...     [jnp.array([5, 5]), jnp.array(3)],\n    ... )\n    &gt;&gt;&gt; stack_one_level(trees, axis=0)\n    [Array([[1, 2], [5, 5]], dtype=int32), Array([4, 3], dtype=int32)]\n</code></pre> Evolved from: link.</p>"},{"location":"tree/Tree/#src.jaxnasium.tree._tree.tree_unstack","title":"<code>tree_unstack(tree, *, axis=0, structure: Optional[PyTreeDef] = None)</code>","text":"<p>Inverse of <code>stack</code>: split a pytree whose leaves were stacked along <code>axis</code> into N separate pytrees.</p> <p>If <code>structure</code> is provided (e.g., from <code>eqx.tree_flatten_one_level</code>), the list of N pytrees is immediately placed back into that container and returned as a single pytree.</p> <p>Arguments:</p> <ul> <li><code>tree</code>: A pytree whose leaves are array-like and all share the same size N along <code>axis</code>.</li> <li><code>axis</code>: The axis that carries the size-N dimension in each leaf (default=0).</li> <li><code>structure</code>: Optional <code>PyTreeDef</code>. If provided, the list of N pytrees is immediately placed back into that container and returned as a single pytree.</li> </ul> <p>Returns:     If <code>structure</code> is <code>None</code>: a list of N pytrees.     Otherwise: a single pytree produced by unflattening <code>structure</code> with those N pytrees.</p> <p>Example: <pre><code>    &gt;&gt;&gt; trees = (\n    ...     [jnp.array([1, 2]), jnp.array(4)],\n    ...     [jnp.array([5, 5]), jnp.array(3)],\n    ... )\n    &gt;&gt;&gt; batched = stack(trees, axis=0)\n    &gt;&gt;&gt; unstack(batched, axis=0)\n    [[Array([1, 2], dtype=int32), Array(4, dtype=int32)],\n     [Array([5, 5], dtype=int32), Array(3, dtype=int32)]]\n</code></pre></p>"},{"location":"tree/Tree/#src.jaxnasium.tree._tree.tree_concatenate","title":"<code>tree_concatenate(trees: PyTree) -&gt; Array</code>","text":"<p>Concatenate the leaves of a pytree into a single 1D array.</p> <pre><code>**Arguments**:\n\n- `trees`: A pytree whose leaves are array-like and all 1d or 0d.\n\n**Returns**: A 1D array containing the concatenated leaves of the pytree.\n\n**Example**:\n</code></pre> <p><code>python     &gt;&gt;&gt; tree = {'a': jnp.array([1, 2]), 'b': jnp.array(3)}     &gt;&gt;&gt; tree_concatenate(tree)     Array([1, 2, 3], dtype=int32)</code></p>"}]}